import json
from tqdm import tqdm
import seaborn as sns
import matplotlib.pyplot as plt

with open('/Users/adamwagnerhoegh/Documents/Legal data/domsdatabasen.retsinformation_newer.json') as f:
    retsinfo = json.load(f)
# Inspecting the structure
retsinfo[0]['kapitler'][0]['paragraffer']
# Inspecting the structure
retsinfo[0]['kapitler']
# Inspecting the structure
retsinfo[0]['kapitler'][0]['paragraffer'][0]['stk'][0]['tekst']
# Inspecting
retsinfo[0]['kapitler'][1]['paragraffer'][0]
retsinfo[0].keys()
retsinfo[0]['shortName']
# Ide: tjek længder af paragraffer. Hvis de er omkring 450 tokens kan det være at det kun er en af gangen der skal retrieves.
# Hvis det er tilfældet kan du måske frasortere alle paragraffer der er længere end 512 minus 50 tokens (context window minus maks længde genereret tekst)

# parse_list = []

# idx = 0

# for lov in tqdm(retsinfo):
#     for kapitel in lov['kapitler']:
#         for paragraffer in kapitel['paragraffer']:
#             temp_paragraf_list = []
#             for styk in paragraffer['stk']:
#                 temp_paragraf_list.append(styk['tekst'])
#             parse_list.append(temp_paragraf_list)

# Ide: tjek længder af paragraffer. Hvis de er omkring 450 tokens kan det være at det kun er en af gangen der skal retrieves.
# Hvis det er tilfældet kan du måske frasortere alle paragraffer der er længere end 512 minus 50 tokens (context window minus maks længde genereret tekst)

rag_list = []

idx = 0

for lov in tqdm(retsinfo):
    for kapitel in lov['kapitler']:
        lov_navn = lov['shortName']
        for paragraffer in kapitel['paragraffer']:
            temp_paragraf_dict = {}
            temp_paragraf_dict['paragraf_nr'] = paragraffer['nummer']
            temp_paragraf_dict['lovnavn'] = lov_navn
            temp_paragraf_list = []
            for styk in paragraffer['stk']:
                temp_paragraf_list.append(styk['tekst'])
            temp_paragraf_dict['text'] = ' '.join(temp_paragraf_list)
            rag_list.append(temp_paragraf_dict)
rag_list[5000]
# Tokenizing all the paragraffer to see how long they generally are

from transformers import AutoTokenizer, T5ForConditionalGeneration

# Load the pretrained T5 model and tokenizer
model_name = "strombergnlp/dant5-large"  
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# Creating a function to tokenize

#max_length = 512-50

def count_tokens(list):
    temp_list = []
    for item in tqdm(list):
        temp_list.append(len(tokenizer(item, return_tensors="pt", padding=True, truncation=True)['input_ids'][0]))
    return temp_list

# Creating the skeleton for the function with an example

len(tokenizer(rag_list[0]['text'], return_tensors="pt")['input_ids'][0])
# Creating a list of the token_lengths

test_list = [item['text'] for item in rag_list]

len_tokens = count_tokens(test_list)
len(rag_list)
# Plotting the distribution of the text token lengths. This is important for RAG, as the documents being retrieved there will be on paragraf-level

fig, ax = plt.subplots()
sns.histplot(len_tokens, ax=ax)
ax.set_xlim(1, 300)
plt.show()
# Saving lists as txt-files

with open("rag_list.txt", "w") as file:
    for item in rag_list:
        file.write(f"{item}\n")
