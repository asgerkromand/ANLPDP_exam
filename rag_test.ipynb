{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/asgerkromand/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from itertools import cycle\n",
    "\n",
    "# For BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate retrieval corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1637/1637 [00:00<00:00, 21308.13it/s]\n"
     ]
    }
   ],
   "source": [
    "path_adam = '/Users/adamwagnerhoegh/Documents/Legal data/domsdatabasen.retsinformation_newer.json'\n",
    "path_asger = \"/Users/asgerkromand/Library/CloudStorage/OneDrive-UniversityofCopenhagen/0. SDS/1 deep learning and nlp/ANLPDP_exam/data/domsdatabasen.retsinformation_newer.json\"\n",
    "path_andreas = '' #missing\n",
    "\n",
    "# Define a function that can cycle through paths the above paths try them out, and yield the path\n",
    "def path():\n",
    "    paths = cycle([path_adam, path_asger, path_andreas])\n",
    "    for path in paths:\n",
    "        if path != '':\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                return data\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            raise FileNotFoundError('No path to data found')\n",
    "\n",
    "retsinfo = path()\n",
    "    \n",
    "rag_list = []\n",
    "idx = 0\n",
    "for lov in tqdm(retsinfo):\n",
    "    for kapitel in lov['kapitler']:\n",
    "        lov_navn = lov['shortName']\n",
    "        for paragraffer in kapitel['paragraffer']:\n",
    "            temp_paragraf_dict = {}\n",
    "            temp_paragraf_dict['paragraf_nr'] = paragraffer['nummer']\n",
    "            temp_paragraf_dict['lovnavn'] = lov_navn\n",
    "            temp_paragraf_list = []\n",
    "            for styk in paragraffer['stk']:\n",
    "                temp_paragraf_list.append(styk['tekst'])\n",
    "            temp_paragraf_dict['text'] = ' '.join(temp_paragraf_list)\n",
    "            rag_list.append(temp_paragraf_dict)\n",
    "\n",
    "with open(\"rag_list.txt\", \"w\") as file:\n",
    "    for item in rag_list:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load excel files in dev set folder\n",
    "import os\n",
    "\n",
    "dev_set_folder = \"devset\"\n",
    "\n",
    "dfs = []\n",
    "for file in os.listdir(dev_set_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(os.path.join(dev_set_folder, file))\n",
    "        dfs.append(df)\n",
    "\n",
    "# merge all excel\n",
    "dev_set = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# add csv\n",
    "rag_batch_1_with_qa = pd.read_csv(\"devset/rag_batch_1_with_qa.csv\", sep=\";\").iloc[:, 1:].dropna()\n",
    "rag_batch_1_with_qa.columns = dev_set.columns\n",
    "dev_set = pd.concat([dev_set, rag_batch_1_with_qa], ignore_index=True)\n",
    "\n",
    "# output dev set\n",
    "dev_set.to_csv(\"devset/dev_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize retrieval corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42593/42593 [00:00<00:00, 61001.76it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_list2 = rag_list\n",
    "\n",
    "def preprocess(rag_list):\n",
    "    # extract and preprocess text\n",
    "    corpus = [item['text'] for item in rag_list]\n",
    "    corpus = [re.sub('\\\\s{2,}', ' ', \n",
    "                     re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                           item.lower())) for item in corpus]\n",
    "\n",
    "    # remove stopwords\n",
    "    #nltk.download('punkt')\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    corpus = [' '.join(word for word in text.split() \n",
    "                      if word not in stop_words) for text in tqdm(corpus)]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = preprocess(rag_list2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval pipeline for BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvor og hvornår skal en erhverver af en eller flere ihændehaveraktier, som besidder mindre end 5 pct. af selskabskapitalens stemmerettigheder eller mindre end 5 pct. af selskabskapitalen, registreres? \n",
      "\n",
      "§ 80 c.: Bidraget til den obligatoriske pensionsordning beregnes med følgende procentsatser: I 2020 med 0,3 pct. I 2020 med 0,3 pct. I 2021 med 0,6 pct. I 2022 med 0,9 pct. I 2023 med 1,2 pct. I 2024 med 1,5 pct. I 2025 med 1,8 pct. I 2026 med 2,1 pct. I 2027 med 2,4 pct. I 2028 med 2,7 pct. I 2029 med 3,0 pct. Fra 2030 med 3,3 pct.\n",
      "§ 55.: Enhver, der besidder kapitalandele i et kapitalselskab, skal give meddelelse herom til selskabet, når kapitalandelenes stemmeret udgør mindst 5 pct. af selskabskapitalens stemmerettigheder eller udgør mindst 5 pct. af selskabskapitalen eller kapitalandelenes stemmeret udgør mindst 5 pct. af selskabskapitalens stemmerettigheder eller udgør mindst 5 pct. af selskabskapitalen eller ændring i et allerede meddelt besiddelsesforhold bevirker, at grænserne på 5, 10, 15, 20, 25, 50, 90 eller 100 pct. og grænserne på 1/3 eller 2/3 af selskabskapitalens stemmerettigheder eller selskabskapitalen nås eller ikke længere er nået. Til besiddelse efter stk. 1 medregnes kapitalandele, hvis stemmeret tilkommer en virksomhed, som den pågældende kontrollerer ved at have den forbindelse, som er nævnt i § 7, og kapitalandele, hvis stemmeret tilkommer en virksomhed, som den pågældende kontrollerer ved at have den forbindelse, som er nævnt i § 7, og kapitalandele, som den pågældende har stillet til sikkerhed, medmindre panthaver råder over stemmeretten og erklærer at have til hensigt at udøve den. Erhvervsstyrelsen kan fastsætte nærmere regler om besiddelse og om meddelelse om besiddelse af kapitalandele efter stk. 1 og 2.\n",
      "§ 57 a.: En erhverver af en eller flere ihændehaveraktier, som besidder mindre end 5 pct. af selskabskapitalens stemmerettigheder eller mindre end 5 pct. af selskabskapitalen, skal senest 2 uger efter erhvervelsen registreres i Erhvervsstyrelsens it-system, jf. dog stk. 4. Registreringen i henhold til stk. 1 skal indeholde oplysning om dato for erhvervelse, antallet af ihændehaveraktier og erhververens fulde navn, bopæl og cpr-nummer eller for virksomheders vedkommende navn, cvr-nummer og hjemsted. Ved overdragelse af en eller flere ihændehaveraktier, som er registreret i medfør af stk. 1, skal det senest 2 uger efter overdragelsen registreres i Erhvervsstyrelsens it-system, at overdrageren ikke længere besidder aktierne, og datoen for overdragelsen skal registreres, jf. dog stk. 4. Pligten til at foretage registrering efter stk. 1 og 3 gælder ikke, hvis de ihændehaveraktier, der er henholdsvis erhvervet og overdraget, er optaget til handel på et reguleret marked. Oplysninger, som er registreret i medfør af stk. 1 og 3, kan alene videregives til andre offentlige myndigheder. Erhvervsstyrelsen fastsætter nærmere regler om registrering af ejeroplysninger om ihændehaveraktier i styrelsens it-system, herunder hvilke oplysninger erhververen eller overdrageren selv kan eller skal registrere i styrelsens it-system.\n"
     ]
    }
   ],
   "source": [
    "def sparse_retrieval(question, sparse_matrix, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # preprocess and vectorize question\n",
    "    question_processed = [re.sub('\\\\s{2,}', ' ', \n",
    "                               re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                                     question.lower()))]\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_processed = [' '.join(word for word in text.split() \n",
    "                                 if word not in stop_words) for text in question_processed]\n",
    "    \n",
    "    question_vector = vectorizer.transform(question_processed)\n",
    "\n",
    "    # sparse retrieval (cosine similarity)\n",
    "    sparse_retrieval = sparse_matrix.dot(question_vector.T).toarray()\n",
    "\n",
    "    # get top k paragraphs\n",
    "    top_k = np.argsort(sparse_retrieval.flatten())[-k:]\n",
    "\n",
    "    return top_k\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k = sparse_retrieval(random_question, X)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list2[i][\"paragraf_nr\"]}: {rag_list2[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hvad skal en virksomhed, som har ansat minimum 10 personer, der gør tjeneste som besætningsmedlemmer på et luftfartøj?\n",
      "\n",
      "§ 40 c.: I virksomheder med 1-9 ansatte, der gør tjeneste som besætningsmedlemmer på luftfartøj, skal virksomhedens arbejde for arbejdsmiljø under denne tjeneste udføres ved personlig kontakt mellem arbejdsgiveren og de ansatte besætningsmedlemmer. (Score: 36.18)\n",
      "§ 40 c.: I virksomheder med 1-9 ansatte, der gør tjeneste som besætningsmedlemmer på luftfartøj, skal virksomhedens arbejde for arbejdsmiljø under denne tjeneste udføres ved personlig kontakt mellem arbejdsgiveren og de ansatte besætningsmedlemmer. (Score: 36.18)\n",
      "§ 40 e.: I virksomheder med 10 ansatte eller derover, som gør tjeneste som besætningsmedlemmer på luftfartøj, oprettes en arbejdsmiljøorganisation. Arbejdsmiljøorganisationen udgøres af højst 4 arbejdsmiljørepræsentanter og et tilsvarende antal repræsentanter for arbejdsgiveren. Arbejdsmiljøorganisationen skal inden for virksomheden være rådgivende med hensyn til planlægning og gennemførelse af foranstaltninger af betydning for arbejdsmiljøet for tjeneste på luftfartøjer. Arbejdsmiljøorganisationen skal mødes mindst 1 gang årligt. (Score: 28.40)\n"
     ]
    }
   ],
   "source": [
    "def bm25_retrieval(question, bm25_model, rag_list, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of the most relevant paragraphs based on BM25.\n",
    "    \"\"\"\n",
    "    # Preprocess and tokenize the question\n",
    "    question_processed = re.sub(r'\\s{2,}', ' ', \n",
    "                                 re.sub(r'\\W|[0-9]|§', ' ', question.lower()))\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_tokens = [word for word in question_processed.split() if word not in stop_words]\n",
    "\n",
    "    # Get BM25 scores for the query\n",
    "    scores = bm25_model.get_scores(question_tokens)\n",
    "\n",
    "    # Get the top k results\n",
    "    top_k_indices = np.argsort(scores)[-k:][::-1]  # Sort scores in descending order\n",
    "\n",
    "    # Return the top k paragraphs\n",
    "    return [(rag_list[i]['paragraf_nr'], rag_list[i]['text'], scores[i]) for i in top_k_indices]\n",
    "\n",
    "# Example Usage\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(f\"Question: {random_question}\\n\")\n",
    "\n",
    "# Assuming bm25 is the initialized BM25Okapi model\n",
    "top_k_results = bm25_retrieval(random_question, bm25, rag_list2, k=3)\n",
    "\n",
    "# Print top-k results\n",
    "for paragraf_nr, text, score in top_k_results:\n",
    "    print(f\"{paragraf_nr}: {text} (Score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating embedding corpus with the 'KennethTM/bert-base-uncased-danish'\n",
    "\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "# bert_model = AutoModel.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# cls_embeddings = []\n",
    "\n",
    "# idx = 0\n",
    "\n",
    "# for item in tqdm(rag_list):\n",
    "#     # doing a try and except as some paragraphs may exceed the context window of the BERT (I believe)\n",
    "#     try:\n",
    "#         # tokenize texts\n",
    "#         input_ids = bert_tokenizer.encode(item['text'], return_tensors='pt')\n",
    "#         # run through BERT\n",
    "#         with torch.no_grad():  # disable gradient computation for inference\n",
    "#             outputs = bert_model(input_ids)\n",
    "#         # extract cls-token\n",
    "#         cls_vector = outputs.last_hidden_state[:, 0, :]\n",
    "#         # add cls-vector to list of embeddings\n",
    "#         cls_embeddings.append(cls_vector)\n",
    "#     except:\n",
    "#         # if error then count errors with this\n",
    "#         idx += 1\n",
    "\n",
    "# print(f'{idx} no. of errors')\n",
    "\n",
    "# # concatenate list into torch tensor\n",
    "# cls_embeddings_tensor = torch.cat(cls_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the tensor\n",
    "#torch.save(cls_embeddings_tensor, '/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tensor\n",
    "\n",
    "# cls_embeddings_tensor = torch.load('/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating embedding corpus with the 'vesteinn/DanskBERT'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "# model = AutoModel.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# cls_embeddings = []\n",
    "\n",
    "# idx = 0\n",
    "\n",
    "# for item in tqdm(rag_list):\n",
    "#     # doing a try and except as some paragraphs may exceed the context window of the BERT (I believe)\n",
    "#     try:\n",
    "#         # tokenize texts\n",
    "#         input_ids = tokenizer.encode(item['text'], return_tensors='pt')\n",
    "#         # run through BERT\n",
    "#         with torch.no_grad():  # disable gradient computation for inference\n",
    "#             outputs = model(input_ids)\n",
    "#         # extract cls-token\n",
    "#         cls_vector = outputs.last_hidden_state[:, 0, :]\n",
    "#         # add cls-vector to list of embeddings\n",
    "#         cls_embeddings.append(cls_vector)\n",
    "#     except:\n",
    "#         # if error then count errors with this\n",
    "#         idx += 1\n",
    "\n",
    "# print(f'{idx} no. of errors')\n",
    "\n",
    "# # concatenate list into torch tensor\n",
    "# cls_embeddings_tensor_DanskBERT = torch.cat(cls_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the DanskBERT-tensor\n",
    "#torch.save(cls_embeddings_tensor_DanskBERT, '/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_DanskBERT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the DanskBERT-tensor\n",
    "#cls_embeddings_tensor_DanskBERT = torch.load('/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_DanskBERT.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at vesteinn/DanskBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "bert_model = AutoModel.from_pretrained(\"vesteinn/DanskBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bedriftværn, der er etableret i henhold til den lovgivning, der er gældende indtil 1. januar 1993, opretholdes, medmindre at hvad? \n",
      "\n",
      "Inden der indgås aftale om køb af en pakkerejse, skal den rejsende gives følgende oplysninger, hvis det er relevant for pakkerejsen, herunder om rejseydelsernes væsentligste kendetegn: Rejsedestination, rejserute, varigheden af og tidspunktet for opholdet og, hvis indkvartering er inkluderet, antallet af inkluderede overnatninger, Rejsedestination, rejserute, varigheden af og tidspunktet for opholdet og, hvis indkvartering er inkluderet, antallet af inkluderede overnatninger, anvendte befordringsmidler og deres kendetegn og kategori, sted, dato og tidspunkt for af- og hjemrejse og oplysninger om steder, hvor der gøres ophold undervejs, med angivelse af disse opholds varighed og transportforbindelser, indkvarteringsstedets beliggenhed, væsentligste kendetegn og kategori i henhold til destinationslandets regler, inkluderede måltider, besøg, udflugter eller andre ydelser, der er inkluderet i pakkerejsens samlede pris, hvorvidt nogen af rejseydelserne leveres til den rejsende som del af en gruppe, hvis det ikke fremgår af sammenhængen, og hvis muligt, gruppens størrelse, hvis den rejsendes nytte af andre turistydelser afhænger af en effektiv mundtlig kommunikation, oplyses det sprog, som disse ydelser vil foregå på, om rejsen generelt er egnet for bevægelseshæmmede personer og på den rejsendes anmodning præcise oplysninger om rejsens egnethed under hensyn til den rejsendes behov, rejsearrangørens og i givet fald formidlerens firmanavn, adresse, telefonnummer og e-mailadresse, hvis den erhvervsdrivende har en sådan, den samlede pris for pakkerejsen inklusive skatter, afgifter og alle yderligere gebyrer og omkostninger eller eventuelt, hvilke typer yderligere omkostninger den rejsende typisk vil kunne forvente at skulle betale, betalingsvilkårene for rejsen, det mindste antal personer, der kræves, for at pakkerejsen gennemføres, og den frist, som gælder for rejsearrangørens opsigelse af pakkerejsen, hvis minimumsantallet ikke er nået, jf. § 20, stk. 1, generelle oplysninger om pas- og visumkrav og oplysninger om sundhedsmæssige formaliteter i de‌sti‌na‌ti‌‌onslandet, den rejsendes afbestillingsret efter § 15, stk. 1, og muligheden for eller forpligtelsen til tegning af en forsikring, der dækker den rejsendes udgifter ved opsigelsen af aftalen eller udgifter til bistand, herunder til hjemtransport i tilfælde af ulykke, sygdom eller død. Den rejsende skal sammen med de oplysninger, der er nævnt i stk. 1, gives de standardoplysninger, der er nævnt i bilag 1, del A og B, ved brug af skemaerne i bilag 1, del A og B. Forbrugeraftalelovens § 8, stk. 3, og § 12 finder anvendelse på pakkerejser, som sælges eller udbydes til rejsende.\n",
      "Arresten kan kræves ophævet af fogeden, hvis kreditor undlader at anlægge arrestsag eller sag vedrørende kravet inden de i § 683 nævnte frister, eller hvis nogen af disse sager afvises eller hæves. Arresten kan ophæves helt eller delvis på grund af omstændigheder, som er indtruffet efter arrestens foretagelse. Forinden arresten ophæves, skal fogeden så vidt muligt give kreditor lejlighed til at udtale sig.\n",
      "For virksomheder, der i medfør af momslovens § 57, stk. 4, anvender kalenderhalvåret som afgiftsperiode i første og andet halvår af 2020, sammenlægges første halvdel af kalenderåret 2020 med anden halvdel af kalenderåret 2020, således at angivelsesfristen for den samlede periode udløber den 1. marts 2021, medmindre virksomheden angiver et momstilsvar for første halvår 2020 senest den 1. september 2020.\n"
     ]
    }
   ],
   "source": [
    "def dense_retrieval(question, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the input sentence\n",
    "    input_ids = bert_tokenizer.encode(question, return_tensors=\"pt\")  # Encode and add batch dimension\n",
    "    # Pass the input through the model\n",
    "    \n",
    "    with torch.no_grad():  # disable gradient computation for inference\n",
    "        outputs = bert_model(input_ids)\n",
    "\n",
    "    # Extract the CLS token representation\n",
    "    cls_vector = outputs.last_hidden_state[:, 0, :]  # CLS token is at position 0\n",
    "    \n",
    "    # sparse retrieval (cosine similarity)\n",
    "    dense_retrieval = cls_embeddings_tensor_DanskBERT @ torch.transpose(cls_vector, 0, 1)\n",
    "    \n",
    "    # get top k paragraphs\n",
    "    top_k_indices = torch.sort(dense_retrieval, descending=True, dim=0)[1][:k]\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k = dense_retrieval(random_question, k=3)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fejl i 6, mangler spørgsmålstegn\n",
    "# 7 er et dårligt spørgsmål\n",
    "# 21 er også lidt dårlig\n",
    "# 35 er ukomplet\n",
    "# 40 og 41 er de samme spørgsmål\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
