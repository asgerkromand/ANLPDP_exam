{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/adamwagnerhoegh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, T5ForConditionalGeneration\n",
    "import torch\n",
    "from itertools import cycle\n",
    "import os \n",
    "from transformers import pipeline\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# For BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate retrieval corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1637/1637 [00:00<00:00, 10544.75it/s]\n"
     ]
    }
   ],
   "source": [
    "path_adam = '/Users/adamwagnerhoegh/Documents/Legal data/domsdatabasen.retsinformation_newer.json'\n",
    "path_asger = \"/Users/asgerkromand/Library/CloudStorage/OneDrive-UniversityofCopenhagen/0. SDS/1 deep learning and nlp/ANLPDP_exam/data/domsdatabasen.retsinformation_newer.json\"\n",
    "path_andreas = '' #missing\n",
    "\n",
    "# Define a function that can cycle through paths the above paths try them out, and yield the path\n",
    "def path():\n",
    "    paths = cycle([path_adam, path_asger, path_andreas])\n",
    "    for path in paths:\n",
    "        if path != '':\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                return data\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            raise FileNotFoundError('No path to data found')\n",
    "\n",
    "retsinfo = path()\n",
    "    \n",
    "rag_list = []\n",
    "idx = 0\n",
    "for lov in tqdm(retsinfo):\n",
    "    for kapitel in lov['kapitler']:\n",
    "        lov_navn = lov['shortName']\n",
    "        for paragraffer in kapitel['paragraffer']:\n",
    "            temp_paragraf_dict = {}\n",
    "            temp_paragraf_dict['paragraf_nr'] = paragraffer['nummer']\n",
    "            temp_paragraf_dict['lovnavn'] = lov_navn\n",
    "            temp_paragraf_list = []\n",
    "            for styk in paragraffer['stk']:\n",
    "                temp_paragraf_list.append(styk['tekst'])\n",
    "            temp_paragraf_dict['text'] = ' '.join(temp_paragraf_list)\n",
    "            rag_list.append(temp_paragraf_dict)\n",
    "\n",
    "with open(\"rag_list.txt\", \"w\") as file:\n",
    "    for item in rag_list:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load excel files in dev set folder\n",
    "import os\n",
    "\n",
    "dev_set_folder = \"devset\"\n",
    "\n",
    "dfs = []\n",
    "for file in os.listdir(dev_set_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(os.path.join(dev_set_folder, file))\n",
    "        dfs.append(df)\n",
    "\n",
    "# merge all excel\n",
    "dev_set = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# add csv\n",
    "rag_batch_1_with_qa = pd.read_csv(\"devset/rag_batch_1_with_qa.csv\", sep=\",\").iloc[:, 1:].dropna()\n",
    "rag_batch_1_with_qa.columns = dev_set.columns\n",
    "dev_set = pd.concat([dev_set, rag_batch_1_with_qa], ignore_index=True)\n",
    "\n",
    "# output dev set\n",
    "dev_set.to_csv(\"devset/dev_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize retrieval corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42593/42593 [00:00<00:00, 115199.02it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_list2 = rag_list\n",
    "\n",
    "def preprocess(rag_list):\n",
    "    # extract and preprocess text\n",
    "    corpus = [item['text'] for item in rag_list]\n",
    "    corpus = [re.sub('\\\\s{2,}', ' ', \n",
    "                     re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                           item.lower())) for item in corpus]\n",
    "\n",
    "    # remove stopwords\n",
    "    #nltk.download('punkt')\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    corpus = [' '.join(word for word in text.split() \n",
    "                      if word not in stop_words) for text in tqdm(corpus)]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = preprocess(rag_list2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval pipeline for BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvad sker der med en kapitalejers stemmeret på generalforsamlingen, hvis kapitalejeren har undladt rettidigt at efterkomme det centrale ledelsesorgans krav om indbetaling af udestående beløb på en kapitalandel? \n",
      "\n",
      "§ 170.: Det centrale ledelsesorgans beslutning i henhold til § 169 skal i sin helhed optages i vedtægterne.\n",
      "§ 52.: Ejerbogen for et kapitalselskab, som har udstedt navnekapitalandele, skal indeholde følgende oplysninger, jf. dog stk. 3: Kapitalejers samlede beholdning af kapitalandele. Kapitalejers samlede beholdning af kapitalandele. Kapitalejers og panthavers navn og bopæl og for virksomheder navn, cvr-nummer og hjemsted, jf. stk. 2. Dato for erhvervelse, afhændelse eller pantsætning, herunder kapitalandelenes størrelse. De stemmerettigheder, der er knyttet til kapitalandelene. Er kapitalejeren eller panthaveren en udenlandsk statsborger eller en udenlandsk juridisk person, skal meddelelsen, jf. § 53, stk. 1, vedlægges anden dokumentation, der sikrer en entydig identifikation af kapitalejeren eller panthaveren. For aktieselskaber, som har udstedt ejerbeviser eller har aktier udstedt gennem en værdipapircentral, finder stk. 1 og 2 ikke anvendelse.\n",
      "§ 34.: En kapitalejers rettigheder efter denne lov består, uanset om kapitalandelen er fuldt indbetalt, jf. dog stk. 3. En kapitalejer har pligt til at foretage indbetaling på en kapitalandel, når det centrale ledelsesorgan anmoder herom, jf. § 33, stk. 2. Har en kapitalejer undladt rettidigt at efterkomme det centrale ledelsesorgans krav om indbetaling af udestående beløb på en kapitalandel, kan kapitalejeren ikke udøve sin stemmeret på generalforsamlingen for nogen del af sin kapitalpost i selskabet, og den pågældendes kapitalpost anses ikke for repræsenteret på generalforsamlingen, før beløbet er indbetalt og registreret i kapitalselskabet. En kapitalejer kan ikke uden samtykke fra kapitalselskabets centrale ledelsesorgan bringe fordringer på kapitalselskabet i modregning mod sin forpligtelse til at indbetale udestående beløb. En kapitalejer kan ikke uden samtykke fra kapitalselskabets centrale ledelsesorgan indskyde andre værdier end kontanter til opfyldelse af sin forpligtelse til at indbetale udestående beløb. Overdrager en kapitalejer en ikke fuldt indbetalt kapitalandel, hæfter vedkommende solidarisk med erhververen og senere erhververe for den resterende indbetaling af kapitalandelen.\n"
     ]
    }
   ],
   "source": [
    "def sparse_retrieval(question, sparse_matrix, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # preprocess and vectorize question\n",
    "    question_processed = [re.sub('\\\\s{2,}', ' ', \n",
    "                               re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                                     question.lower()))]\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_processed = [' '.join(word for word in text.split() \n",
    "                                 if word not in stop_words) for text in question_processed]\n",
    "    \n",
    "    question_vector = vectorizer.transform(question_processed)\n",
    "\n",
    "    # sparse retrieval (cosine similarity)\n",
    "    sparse_retrieval = sparse_matrix.dot(question_vector.T).toarray()\n",
    "\n",
    "    # get top k paragraphs\n",
    "    top_k_indices = np.argsort(sparse_retrieval.flatten())[-k:]\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k_indices = sparse_retrieval(random_question, X)\n",
    "for i in top_k_indices:\n",
    "    print(f'{rag_list2[i][\"paragraf_nr\"]}: {rag_list2[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bm25_retrieval(question, bm25_model, rag_list, k=3):\n",
    "#     \"\"\"\n",
    "#     Function that takes a question and returns a list of the most relevant paragraphs based on BM25.\n",
    "#     \"\"\"\n",
    "#     # Preprocess and tokenize the question\n",
    "#     question_processed = re.sub(r'\\s{2,}', ' ', \n",
    "#                                  re.sub(r'\\W|[0-9]|§', ' ', question.lower()))\n",
    "#     stop_words = set(stopwords.words('danish'))\n",
    "#     question_tokens = [word for word in question_processed.split() if word not in stop_words]\n",
    "\n",
    "#     # Get BM25 scores for the query\n",
    "#     scores = bm25_model.get_scores(question_tokens)\n",
    "\n",
    "#     # Get the top k results\n",
    "#     top_k_indices = np.argsort(scores)[-k:][::-1]  # Sort scores in descending order\n",
    "\n",
    "#     # Return the top k paragraphs\n",
    "#     return [(rag_list[i]['paragraf_nr'], rag_list[i]['text'], scores[i]) for i in top_k_indices]\n",
    "\n",
    "# # Example Usage\n",
    "# random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "# print(f\"Question: {random_question}\\n\")\n",
    "\n",
    "# # Assuming bm25 is the initialized BM25Okapi model\n",
    "# top_k_results = bm25_retrieval(random_question, bm25, rag_list2, k=3)\n",
    "\n",
    "# # Print top-k results\n",
    "# for paragraf_nr, text, score in top_k_results:\n",
    "#     print(f\"{paragraf_nr}: {text} (Score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embedding_matrix(pooling, save=False, save_folder=None):\n",
    "    # initialise model\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "    bert_model = AutoModel.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "\n",
    "    # define device\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # move model to device\n",
    "    bert_model.to(device)\n",
    "\n",
    "    # create list of embedding vectors to concatenate into a torch tensor\n",
    "    embeddings = []\n",
    "\n",
    "    # index to track numer of mistakes\n",
    "    idx = 0\n",
    "\n",
    "    for item in tqdm(rag_list):\n",
    "        # doing a try and except as some paragraphs may exceed the context window of the BERT (I believe)\n",
    "        try:\n",
    "            # tokenize texts\n",
    "            input_ids = bert_tokenizer.encode(item['text'], return_tensors='pt').to(device)\n",
    "            # run through BERT\n",
    "            with torch.no_grad():  # disable gradient computation for inference\n",
    "                outputs = bert_model(input_ids)\n",
    "            \n",
    "            # different kinds of pooling\n",
    "            if pooling == 'cls':\n",
    "                embedding_vector = outputs.last_hidden_state[:, 0, :]\n",
    "            elif pooling == 'max':\n",
    "                embedding_vector = torch.max(outputs, dim=1)[0]\n",
    "            elif pooling == 'mean':\n",
    "                embedding_vector = torch.mean(outputs, dim=1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling method: {pooling}\")\n",
    "            \n",
    "            # add cls-vector to list of embeddings\n",
    "            embeddings.append(embedding_vector)\n",
    "        except:\n",
    "            # if error then count errors with this\n",
    "            embeddings.append(torch.zeros(1, 768))\n",
    "            idx += 1\n",
    "\n",
    "    print(f'{idx} no. of errors')\n",
    "\n",
    "    # concatenate list into torch tensor\n",
    "    embeddings_tensor = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    if save == True:\n",
    "        # make sure that folder exists\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "        # save tensor \n",
    "        torch.save(embeddings_tensor, f'{save_folder}/{pooling}_embeddings_tensor.pt')\n",
    "\n",
    "    return embeddings_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_embedding_matrix(pooling='cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at vesteinn/DanskBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "bert_model = AutoModel.from_pretrained(\"vesteinn/DanskBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can load cls or max respectively, mean still needs to be created\n",
    "embeddings_matrix = torch.load('/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_DanskBERT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvad kan et jobcenter give et tilbud om, udover ansættelse med løntilskud, nytteindsats samt vejledning og opkvalificering? \n",
      "\n",
      "Jobcenteret kan give tilbud om følgende: Virksomhedspraktik, jf. kapitel 11. Virksomhedspraktik, jf. kapitel 11. Ansættelse med løntilskud, jf. kapitel 12. Nytteindsats, jf. kapitel 13. Vejledning og opkvalificering, jf. kapitel 14. Tilbud kan gives hver for sig eller i kombination.\n",
      "Forud for ansættelse med løntilskud skal spørgsmålet om ansættelsen have været drøftet mellem arbejdsgiveren og en repræsentant for de ansatte hos arbejdsgiveren.\n",
      "Det er en betingelse for retten til jobpræmie, at personen er registreret i CPR-registeret med bopæl i Danmark.\n"
     ]
    }
   ],
   "source": [
    "def dense_retrieval(question, pooling='cls', k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    pooling = 'cls', 'max' or 'mean'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the input sentence\n",
    "    input_ids = bert_tokenizer.encode(question, return_tensors=\"pt\")  # Encode and add batch dimension\n",
    "    # Pass the input through the model\n",
    "    \n",
    "    with torch.no_grad():  # disable gradient computation for inference\n",
    "        outputs = bert_model(input_ids)\n",
    "    \n",
    "    if pooling == 'cls':\n",
    "        # Extract the CLS token representation\n",
    "        embedding_vector = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    elif pooling == 'max':\n",
    "        embedding_vector = torch.max(outputs.last_hidden_state, dim=1)[0]\n",
    "\n",
    "    elif pooling == 'mean':\n",
    "        embedding_vector = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    \n",
    "    # normalise the cls-embedding and the embedding matrix so that the dot product\n",
    "    # below is now cosine similarity\n",
    "    embedding_vector_normalised = embedding_vector / torch.norm(embedding_vector, dim=-1, keepdim=True)\n",
    "    embeddings_matrix_normalised = embeddings_matrix / torch.norm(embeddings_matrix, dim=-1, keepdim=True)\n",
    "\n",
    "    # finding most similar vectors with dot product\n",
    "    dense_retrieval = embeddings_matrix_normalised @ torch.transpose(embedding_vector_normalised, 0, 1)\n",
    "    \n",
    "    # sorting retrieved embeddings. this leaves a bunch of nan-values at the top\n",
    "    sorted_retrieval = torch.sort(dense_retrieval, descending=True, stable=True, dim=0)\n",
    "\n",
    "    # recreating list, sorting out nan-values\n",
    "    fixed_retrieval_list = [(item, idx) for (item, idx) in zip(sorted_retrieval[0], sorted_retrieval[1]) if torch.isnan(item) == False]\n",
    "\n",
    "    # get top k paragraphs\n",
    "    top_k_indices = [item[1] for item in fixed_retrieval_list[:k]]\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n') \n",
    "top_k = dense_retrieval(random_question, pooling='cls', k=3)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is a bit broken, don't think we should use it\n",
    "\n",
    "# def evaluate(model_name, retrieval_method, metric, k):\n",
    "#     \"\"\"\n",
    "#     model_name = 'KennethTM/gpt-neo-1.3B-danish' or 'strombergnlp/dant5-large'\n",
    "#     retrieval_method = 'tf-idf', 'bm25' or 'dense'\n",
    "#     metric = 'bleu', 'rouge' or 'meteor'\n",
    "#     \"\"\"\n",
    "#     # set device to mps\n",
    "#     device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "#     # load AutoTokenizer for model\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#     # generate list of answers to fill\n",
    "#     answers = []\n",
    "\n",
    "#     # load neo\n",
    "#     # the loops are made for each model to not waste compute on loading models for each question\n",
    "\n",
    "#     if model_name == 'KennethTM/gpt-neo-1.3B-danish':\n",
    "#         model = AutoModelForCausalLM.from_pretrained(\"KennethTM/gpt-neo-1.3B-danish\").to(device)\n",
    "    \n",
    "#         for question in tqdm(dev_set['question, str'], desc='Answering questions with neo'):\n",
    "\n",
    "#             if retrieval_method == 'tf-idf':\n",
    "#                 # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "#                 paragraphs = [rag_list[i]['text'] for i in sparse_retrieval(question, X, k=3)]\n",
    "#                 # join list into long string\n",
    "#                 paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "#             elif retrieval_method == 'bm25':\n",
    "#                 paragraphs = bm25_retrieval(rag_list=rag_list, k=3)\n",
    "            \n",
    "#             elif retrieval_method == 'dense_retrieval':\n",
    "#                 # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "#                 paragraphs = [rag_list[i]['text'] for i in dense_retrieval(question, k=3)]\n",
    "#                 # join list into long string\n",
    "#                 paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "#             # assemble all in prompt\n",
    "#             prompt = f'Kontekst: {paragraphs} Spørgsmål: {question} Svar: '\n",
    "\n",
    "#             # tokenize\n",
    "#             input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#             # set max_length to no. of tokens in prompt + 100 (the 100 are thus for generation)\n",
    "#             max_length = int(input_ids['input_ids'].size(1)) + 100\n",
    "\n",
    "#             # generate answer with no_grad() to save compute\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model.generate(\n",
    "#                     input_ids,\n",
    "#                     max_length=max_length,\n",
    "#                     pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "#             # decode the generated answer\n",
    "#             answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#             answers.append(answer)\n",
    "    \n",
    "\n",
    "#     # load T5\n",
    "#     elif model_name == 'strombergnlp/dant5-large':\n",
    "#         model = T5ForConditionalGeneration.from_pretrained(\"strombergnlp/dant5-large\").to(device)\n",
    "\n",
    "#         for question in tqdm(dev_set['question, str'], desc='Answering questions with T5'):\n",
    "\n",
    "#             if retrieval_method == 'tf-idf':\n",
    "#                 # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "#                 paragraphs = [rag_list[i]['text'] for i in sparse_retrieval(question, X, k=3)]\n",
    "#                 # join list into long string\n",
    "#                 paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "#             elif retrieval_method == 'bm25':\n",
    "#                 paragraphs = bm25_retrieval(rag_list=rag_list, k=3)\n",
    "            \n",
    "#             elif retrieval_method == 'dense_retrieval':\n",
    "#                 # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "#                 paragraphs = [rag_list[i]['text'] for i in dense_retrieval(question, k=3)]\n",
    "#                 # join list into long string\n",
    "#                 paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "#             # assemble all in input\n",
    "#             input_text = f\"Spørgsmål: {question} Kontekst: {paragraphs} Svar:\"\n",
    "\n",
    "#             # tokenize\n",
    "#             input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "#             # generate answer with no_grad() to save compute\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model.generate(\n",
    "#                     input_ids,\n",
    "#                     max_length=max_length,\n",
    "#                     pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model.generate(input_ids, max_length=100)\n",
    "\n",
    "#             # Decode and print the generated answer\n",
    "#             answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#             answers.append(answer)\n",
    "        \n",
    "#     # choosing metric to evaluate answers\n",
    "#     if metric == 'bleu':\n",
    "#         scores = []\n",
    "#         idx = 0\n",
    "\n",
    "#         for pred_answer, true_answer in tqdm(zip(answers, list(dev_set['answer, str'])), desc='Evaluating with bleu'):\n",
    "#             try:\n",
    "#                 scores.append(nltk.translate.bleu_score.sentence_bleu([true_answer], pred_answer))\n",
    "#             except:\n",
    "#                 print(f'Error when computing bleu-score at index {idx}')\n",
    "#             idx += 1\n",
    "    \n",
    "#     elif metric == 'meteor':\n",
    "#         scores = []\n",
    "#         idx = 0\n",
    "\n",
    "#         for pred_answer, true_answer in tqdm(zip(answers, list(dev_set['answer, str'])), desc='Evaluating with meteor'):\n",
    "#             try:\n",
    "#                 scores.append(nltk.tranlsate.meteor_score([true_answer], pred_answer))\n",
    "#             except:\n",
    "#                 print(f'Error when computing meteor-score at index {idx}')\n",
    "#             idx += 1\n",
    "\n",
    "#     elif metric == 'rouge':\n",
    "#         scores = []\n",
    "#         idx = 0\n",
    "\n",
    "#         scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "#         for pred_answer, true_answer in tqdm(zip(answers, list(dev_set['answer, str'])), desc='Evaluating with rouge'):\n",
    "#             try:\n",
    "#                 scores.append(score.score(true_answer, pred_answer)['rouge1'])\n",
    "#             except:\n",
    "#                 print(f'Error when computing meteor-score at index {idx}')\n",
    "#             idx += 1\n",
    "\n",
    "#     print(f'{metric}-scores for {model_name} using {retrieval_method}: {np.mean(scores)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create columns for retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF, k=1: 100%|██████████| 106/106 [00:00<00:00, 292.20it/s]\n",
      "TF-IDF, k=2: 100%|██████████| 106/106 [00:00<00:00, 295.00it/s]\n",
      "TF-IDF, k=3: 100%|██████████| 106/106 [00:00<00:00, 293.51it/s]\n",
      "Dense CLS, k=1: 100%|██████████| 106/106 [00:26<00:00,  3.96it/s]\n",
      "Dense CLS, k=2: 100%|██████████| 106/106 [00:27<00:00,  3.90it/s]\n",
      "Dense CLS, k=3: 100%|██████████| 106/106 [00:28<00:00,  3.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# looks like the dense retrievers are generally retrieving shorter paragraphs\n",
    "\n",
    "tf_idf_k1 = []\n",
    "for question in tqdm(dev_set['question, str'], desc='TF-IDF, k=1'):\n",
    "    # prepended documents:\n",
    "    documents = ' '.join([rag_list[i]['text'] for i in sparse_retrieval(question, X, k=1)])\n",
    "    tf_idf_k1.append(documents)\n",
    "\n",
    "tf_idf_k2 = []\n",
    "for question in tqdm(dev_set['question, str'], desc='TF-IDF, k=2'):\n",
    "    # prepended documents:\n",
    "    documents = ' '.join([rag_list[i]['text'] for i in sparse_retrieval(question, X, k=2)])\n",
    "    tf_idf_k2.append(documents)\n",
    "\n",
    "tf_idf_k3 = []\n",
    "for question in tqdm(dev_set['question, str'], desc='TF-IDF, k=3'):\n",
    "    # prepended documents:\n",
    "    documents = ' '.join([rag_list[i]['text'] for i in sparse_retrieval(question, X, k=3)])\n",
    "    tf_idf_k3.append(documents)\n",
    "\n",
    "# insert BM25\n",
    "\n",
    "dense_cls_k1 = []\n",
    "for question in tqdm(dev_set['question, str'], desc='Dense CLS, k=1'):\n",
    "    # prepended documents:\n",
    "    documents = ' '.join([rag_list[i]['text'] for i in dense_retrieval(question, pooling='cls', k=1)])\n",
    "    dense_cls_k1.append(documents)\n",
    "\n",
    "dense_cls_k2 = []\n",
    "for question in tqdm(dev_set['question, str'], desc='Dense CLS, k=2'):\n",
    "    # prepended documents:\n",
    "    documents = ' '.join([rag_list[i]['text'] for i in dense_retrieval(question, pooling='cls', k=2)])\n",
    "    dense_cls_k2.append(documents)\n",
    "\n",
    "dense_cls_k3 = []\n",
    "for question in tqdm(dev_set['question, str'], desc='Dense CLS, k=3'):\n",
    "    # prepended documents:\n",
    "    documents = ' '.join([rag_list[i]['text'] for i in dense_retrieval(question, pooling='cls', k=3)])\n",
    "    dense_cls_k3.append(documents)\n",
    "\n",
    "dense_max_k1 = []\n",
    "for question in tqdm(dev_set['question, str'], desc='Dense max, k=1'):\n",
    "    # prepended documents:\n",
    "    documents = ' '.join([rag_list[i]['text'] for i in dense_retrieval(question, pooling='max', k=1)])\n",
    "    dense_max_k1.append(documents)\n",
    "\n",
    "dense_max_k2 = []\n",
    "for question in tqdm(dev_set['question, str'], desc='Dense max, k=2'):\n",
    "    # prepended documents:\n",
    "    documents = ' '.join([rag_list[i]['text'] for i in dense_retrieval(question, pooling='max', k=2)])\n",
    "    dense_max_k2.append(documents)\n",
    "\n",
    "dense_max_k3 = []\n",
    "for question in tqdm(dev_set['question, str'], desc='Dense max, k=3'):\n",
    "    # prepended documents:\n",
    "    documents = ' '.join([rag_list[i]['text'] for i in dense_retrieval(question, pooling='max', k=3)])\n",
    "    dense_max_k3.append(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding to dev set and saving locally\n",
    "dev_set['tf_idf_k1'] = tf_idf_k1\n",
    "dev_set['tf_idf_k2'] = tf_idf_k2\n",
    "dev_set['tf_idf_k3'] = tf_idf_k3\n",
    "dev_set['dense_cls_k1'] = dense_cls_k1\n",
    "dev_set['dense_cls_k2'] = dense_cls_k2\n",
    "dev_set['dense_cls_k3'] = dense_cls_k3\n",
    "dev_set['dense_max_k1'] = dense_max_k1\n",
    "dev_set['dense_max_k2'] = dense_max_k2\n",
    "dev_set['dense_max_k3'] = dense_max_k3\n",
    "\n",
    "dev_set.to_csv(\"devset/dev_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 2048)\n",
       "    (wpe): Embedding(2048, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "MODEL_NAME = \"KennethTM/gpt-neo-1.3B-danish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set the device\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering questions: 100%|██████████| 106/106 [13:33<00:00,  7.67s/it]\n"
     ]
    }
   ],
   "source": [
    "neo_answers_k1 = []\n",
    "# evaluating tf-idf\n",
    "for question in tqdm(dev_set['question, str'], desc='Answering questions'):\n",
    "    # prepended documents:\n",
    "    documents = ' '.join([rag_list[i]['text'] for i in sparse_retrieval(question, X, k=1)])\n",
    "\n",
    "    # assemble a prompt from the documents, question and prompting an answer\n",
    "    prompt = f\"Kontekst: {documents} Spørgsmål: {question}\\nSvar:\"\n",
    "\n",
    "    # tokenize\n",
    "    input_ids = tokenizer(prompt, padding=True, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "\n",
    "    max_length = len(input_ids[0]) + 100\n",
    "\n",
    "    # generate an answer within torch.no_grad() to save compute\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            # generation set to stop at '.' as it otherwise just repeats itself (think it's because we don't sample)\n",
    "            eos_token_id=tokenizer.convert_tokens_to_ids(\".\")\n",
    "        )\n",
    "\n",
    "    # decode generated answer\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # append answer to list\n",
    "    neo_answers_k1.append(answer[len(prompt):].strip())  # strip the prompt to leave just the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set['neo_answers_k1'] = neo_answers_k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
