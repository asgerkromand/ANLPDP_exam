{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/adamwagnerhoegh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from itertools import cycle\n",
    "import os \n",
    "\n",
    "# For BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate retrieval corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1637/1637 [00:00<00:00, 11455.97it/s]\n"
     ]
    }
   ],
   "source": [
    "path_adam = '/Users/adamwagnerhoegh/Documents/Legal data/domsdatabasen.retsinformation_newer.json'\n",
    "path_asger = \"/Users/asgerkromand/Library/CloudStorage/OneDrive-UniversityofCopenhagen/0. SDS/1 deep learning and nlp/ANLPDP_exam/data/domsdatabasen.retsinformation_newer.json\"\n",
    "path_andreas = '' #missing\n",
    "\n",
    "# Define a function that can cycle through paths the above paths try them out, and yield the path\n",
    "def path():\n",
    "    paths = cycle([path_adam, path_asger, path_andreas])\n",
    "    for path in paths:\n",
    "        if path != '':\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                return data\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            raise FileNotFoundError('No path to data found')\n",
    "\n",
    "retsinfo = path()\n",
    "    \n",
    "rag_list = []\n",
    "idx = 0\n",
    "for lov in tqdm(retsinfo):\n",
    "    for kapitel in lov['kapitler']:\n",
    "        lov_navn = lov['shortName']\n",
    "        for paragraffer in kapitel['paragraffer']:\n",
    "            temp_paragraf_dict = {}\n",
    "            temp_paragraf_dict['paragraf_nr'] = paragraffer['nummer']\n",
    "            temp_paragraf_dict['lovnavn'] = lov_navn\n",
    "            temp_paragraf_list = []\n",
    "            for styk in paragraffer['stk']:\n",
    "                temp_paragraf_list.append(styk['tekst'])\n",
    "            temp_paragraf_dict['text'] = ' '.join(temp_paragraf_list)\n",
    "            rag_list.append(temp_paragraf_dict)\n",
    "\n",
    "with open(\"rag_list.txt\", \"w\") as file:\n",
    "    for item in rag_list:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load excel files in dev set folder\n",
    "import os\n",
    "\n",
    "dev_set_folder = \"devset\"\n",
    "\n",
    "dfs = []\n",
    "for file in os.listdir(dev_set_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(os.path.join(dev_set_folder, file))\n",
    "        dfs.append(df)\n",
    "\n",
    "# merge all excel\n",
    "dev_set = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# add csv\n",
    "rag_batch_1_with_qa = pd.read_csv(\"devset/rag_batch_1_with_qa.csv\", sep=\",\").iloc[:, 1:].dropna()\n",
    "rag_batch_1_with_qa.columns = dev_set.columns\n",
    "dev_set = pd.concat([dev_set, rag_batch_1_with_qa], ignore_index=True)\n",
    "\n",
    "# output dev set\n",
    "dev_set.to_csv(\"devset/dev_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize retrieval corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42593/42593 [00:00<00:00, 115716.35it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_list2 = rag_list\n",
    "\n",
    "def preprocess(rag_list):\n",
    "    # extract and preprocess text\n",
    "    corpus = [item['text'] for item in rag_list]\n",
    "    corpus = [re.sub('\\\\s{2,}', ' ', \n",
    "                     re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                           item.lower())) for item in corpus]\n",
    "\n",
    "    # remove stopwords\n",
    "    #nltk.download('punkt')\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    corpus = [' '.join(word for word in text.split() \n",
    "                      if word not in stop_words) for text in tqdm(corpus)]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = preprocess(rag_list2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval pipeline for BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvilke betingelser skal være opfyldt for at få udbetalt rejse- eller befordringsgodtgørelser efter ligningslovens § 9 A eller 9 B? \n",
      "\n",
      "§ 67 a.: De transaktioner, der på de betingelser, der er fastsat i § 67, stk. 1, udføres af rejsebureauet med henblik på gennemførelse af en rejse, anses som en enkelt ydelse.\n",
      "§ 16.: Personer, som den 23. februar 2005 ikke var afskåret fra at anvende ligningslovens § 9 A, stk. 1-9, jf. ligningslovens § 9 A, stk. 11, og som kan foretage fradrag efter § 3, kan vælge fortsat at være omfattet af ligningslovens § 9 A, stk. 1-9.\n",
      "§ 4.: Når der foretages fradrag efter § 3, kan der ikke samtidig foretages fradrag efter ligningslovens § 9, stk. 1, ligningslovens §§ 9 B-9 D og ligningslovens § 13 samt efter pensionsbeskatningslovens § 49, stk. 1. Personer, som kan foretage fradrag efter § 3, er ikke omfattet af ligningslovens § 9 A, stk. 1-9.\n"
     ]
    }
   ],
   "source": [
    "def sparse_retrieval(question, sparse_matrix, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # preprocess and vectorize question\n",
    "    question_processed = [re.sub('\\\\s{2,}', ' ', \n",
    "                               re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                                     question.lower()))]\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_processed = [' '.join(word for word in text.split() \n",
    "                                 if word not in stop_words) for text in question_processed]\n",
    "    \n",
    "    question_vector = vectorizer.transform(question_processed)\n",
    "\n",
    "    # sparse retrieval (cosine similarity)\n",
    "    sparse_retrieval = sparse_matrix.dot(question_vector.T).toarray()\n",
    "\n",
    "    # get top k paragraphs\n",
    "    top_k = np.argsort(sparse_retrieval.flatten())[-k:]\n",
    "\n",
    "    return top_k\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k = sparse_retrieval(random_question, X)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list2[i][\"paragraf_nr\"]}: {rag_list2[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hvad har Rådet for Dyreforsøg til enhver tid uden retskendelse mod behørig legitimation adgang til?\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bm25' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Assuming bm25 is the initialized BM25Okapi model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m top_k_results \u001b[38;5;241m=\u001b[39m bm25_retrieval(random_question, \u001b[43mbm25\u001b[49m, rag_list2, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Print top-k results\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paragraf_nr, text, score \u001b[38;5;129;01min\u001b[39;00m top_k_results:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bm25' is not defined"
     ]
    }
   ],
   "source": [
    "def bm25_retrieval(question, bm25_model, rag_list, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of the most relevant paragraphs based on BM25.\n",
    "    \"\"\"\n",
    "    # Preprocess and tokenize the question\n",
    "    question_processed = re.sub(r'\\s{2,}', ' ', \n",
    "                                 re.sub(r'\\W|[0-9]|§', ' ', question.lower()))\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_tokens = [word for word in question_processed.split() if word not in stop_words]\n",
    "\n",
    "    # Get BM25 scores for the query\n",
    "    scores = bm25_model.get_scores(question_tokens)\n",
    "\n",
    "    # Get the top k results\n",
    "    top_k_indices = np.argsort(scores)[-k:][::-1]  # Sort scores in descending order\n",
    "\n",
    "    # Return the top k paragraphs\n",
    "    return [(rag_list[i]['paragraf_nr'], rag_list[i]['text'], scores[i]) for i in top_k_indices]\n",
    "\n",
    "# Example Usage\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(f\"Question: {random_question}\\n\")\n",
    "\n",
    "# Assuming bm25 is the initialized BM25Okapi model\n",
    "top_k_results = bm25_retrieval(random_question, bm25, rag_list2, k=3)\n",
    "\n",
    "# Print top-k results\n",
    "for paragraf_nr, text, score in top_k_results:\n",
    "    print(f\"{paragraf_nr}: {text} (Score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embedding_matrix(pooling, save=True, save_folder=None):\n",
    "    # initialise model\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "    bert_model = AutoModel.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "\n",
    "    # define device\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # move model to device\n",
    "    bert_model.to(device)\n",
    "\n",
    "    # create list of embedding vectors to concatenate into a torch tensor\n",
    "    embeddings = []\n",
    "\n",
    "    # index to track numer of mistakes\n",
    "    idx = 0\n",
    "\n",
    "    for item in tqdm(rag_list):\n",
    "        # doing a try and except as some paragraphs may exceed the context window of the BERT (I believe)\n",
    "        try:\n",
    "            # tokenize texts\n",
    "            input_ids = bert_tokenizer.encode(item['text'], return_tensors='pt').to(device)\n",
    "            # run through BERT\n",
    "            with torch.no_grad():  # disable gradient computation for inference\n",
    "                outputs = bert_model(input_ids)\n",
    "            \n",
    "            # different kinds of pooling\n",
    "            if pooling == 'cls':\n",
    "                embedding_vector = outputs.last_hidden_state[:, 0, :]\n",
    "            elif pooling == 'max':\n",
    "                embedding_vector = torch.max(outputs, dim=1)[0]\n",
    "            elif pooling == 'mean':\n",
    "                embedding_vector = torch.mean(outputs, dim=1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling method: {pooling}\")\n",
    "            \n",
    "            # add cls-vector to list of embeddings\n",
    "            embeddings.append(embedding_vector)\n",
    "        except:\n",
    "            # if error then count errors with this\n",
    "            idx += 1\n",
    "\n",
    "    print(f'{idx} no. of errors')\n",
    "\n",
    "    # concatenate list into torch tensor\n",
    "    embeddings_tensor = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    if save == True:\n",
    "        # make sure that folder exists\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "        # save tensor \n",
    "        torch.save(embeddings_tensor, f'{save_folder}/{pooling}_embeddings_tensor.pt')\n",
    "\n",
    "    return embeddings_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_embedding_matrix(pooling='cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at vesteinn/DanskBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "bert_model = AutoModel.from_pretrained(\"vesteinn/DanskBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = torch.load('/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_DanskBERT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvad skal en virksomhed, som har ansat minimum 10 personer, der gør tjeneste som besætningsmedlemmer på et luftfartøj? \n",
      "\n",
      "Hvis en virksomheds tilsvar af skatter og afgifter m.v., der opkræves efter reglerne i denne lov, for en afregningsperiode er negativt, udbetales beløbet til virksomheden. Såfremt angivelsen henholdsvis indberetningen af beløb omfattet af § 2, stk. 1, 4. pkt. er modtaget rettidigt, sker udbetaling efter stk. 1 senest 3 uger efter modtagelsen af angivelsen henholdsvis indberetningen for den pågældende periode. Kan told- og skatteforvaltningen på grund af virksomhedens forhold ikke foretage kontrol af angivelsen eller indberetningen af beløb omfattet af § 2, stk. 1, 4. pkt., afbrydes udbetalingsfristen, indtil virksomhedens forhold ikke længere hindrer kontrol. Beløb, der skulle have været udbetalt efter stk. 1, kan tilbageholdes, såfremt angivelser eller indberetningen af beløb omfattet af § 2, stk. 1, 4. pkt. vedrørende afsluttede afregningsperioder ikke er indgivet. Negative tilsvar efter stk. 1, der indgår ved en samlet kontoopgørelse af virksomhedens tilsvar af skatter og afgifter m.v. efter reglerne i denne lovs kapitel 5, kan alene udbetales, hvis det negative tilsvar modsvares af en kreditsaldo opgjort efter § 16 a, stk. 2, 2. pkt.\n",
      "Inden der indgås aftale om køb af en pakkerejse, skal den rejsende gives følgende oplysninger, hvis det er relevant for pakkerejsen, herunder om rejseydelsernes væsentligste kendetegn: Rejsedestination, rejserute, varigheden af og tidspunktet for opholdet og, hvis indkvartering er inkluderet, antallet af inkluderede overnatninger, Rejsedestination, rejserute, varigheden af og tidspunktet for opholdet og, hvis indkvartering er inkluderet, antallet af inkluderede overnatninger, anvendte befordringsmidler og deres kendetegn og kategori, sted, dato og tidspunkt for af- og hjemrejse og oplysninger om steder, hvor der gøres ophold undervejs, med angivelse af disse opholds varighed og transportforbindelser, indkvarteringsstedets beliggenhed, væsentligste kendetegn og kategori i henhold til destinationslandets regler, inkluderede måltider, besøg, udflugter eller andre ydelser, der er inkluderet i pakkerejsens samlede pris, hvorvidt nogen af rejseydelserne leveres til den rejsende som del af en gruppe, hvis det ikke fremgår af sammenhængen, og hvis muligt, gruppens størrelse, hvis den rejsendes nytte af andre turistydelser afhænger af en effektiv mundtlig kommunikation, oplyses det sprog, som disse ydelser vil foregå på, om rejsen generelt er egnet for bevægelseshæmmede personer og på den rejsendes anmodning præcise oplysninger om rejsens egnethed under hensyn til den rejsendes behov, rejsearrangørens og i givet fald formidlerens firmanavn, adresse, telefonnummer og e-mailadresse, hvis den erhvervsdrivende har en sådan, den samlede pris for pakkerejsen inklusive skatter, afgifter og alle yderligere gebyrer og omkostninger eller eventuelt, hvilke typer yderligere omkostninger den rejsende typisk vil kunne forvente at skulle betale, betalingsvilkårene for rejsen, det mindste antal personer, der kræves, for at pakkerejsen gennemføres, og den frist, som gælder for rejsearrangørens opsigelse af pakkerejsen, hvis minimumsantallet ikke er nået, jf. § 20, stk. 1, generelle oplysninger om pas- og visumkrav og oplysninger om sundhedsmæssige formaliteter i de‌sti‌na‌ti‌‌onslandet, den rejsendes afbestillingsret efter § 15, stk. 1, og muligheden for eller forpligtelsen til tegning af en forsikring, der dækker den rejsendes udgifter ved opsigelsen af aftalen eller udgifter til bistand, herunder til hjemtransport i tilfælde af ulykke, sygdom eller død. Den rejsende skal sammen med de oplysninger, der er nævnt i stk. 1, gives de standardoplysninger, der er nævnt i bilag 1, del A og B, ved brug af skemaerne i bilag 1, del A og B. Forbrugeraftalelovens § 8, stk. 3, og § 12 finder anvendelse på pakkerejser, som sælges eller udbydes til rejsende.\n",
      "Arresten kan kræves ophævet af fogeden, hvis kreditor undlader at anlægge arrestsag eller sag vedrørende kravet inden de i § 683 nævnte frister, eller hvis nogen af disse sager afvises eller hæves. Arresten kan ophæves helt eller delvis på grund af omstændigheder, som er indtruffet efter arrestens foretagelse. Forinden arresten ophæves, skal fogeden så vidt muligt give kreditor lejlighed til at udtale sig.\n"
     ]
    }
   ],
   "source": [
    "def dense_retrieval(question, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the input sentence\n",
    "    input_ids = bert_tokenizer.encode(question, return_tensors=\"pt\")  # Encode and add batch dimension\n",
    "    # Pass the input through the model\n",
    "    \n",
    "    with torch.no_grad():  # disable gradient computation for inference\n",
    "        outputs = bert_model(input_ids)\n",
    "\n",
    "    # Extract the CLS token representation\n",
    "    cls_vector = outputs.last_hidden_state[:, 0, :]  # CLS token is at position 0\n",
    "    \n",
    "    # sparse retrieval (cosine similarity)\n",
    "    dense_retrieval = embeddings_matrix @ torch.transpose(cls_vector, 0, 1)\n",
    "    \n",
    "    # get top k paragraphs\n",
    "    top_k_indices = torch.sort(dense_retrieval, descending=True, dim=0)[1][:k]\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k = dense_retrieval(random_question, k=3)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list[i][\"text\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
