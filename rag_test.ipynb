{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/adamwagnerhoegh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, T5ForConditionalGeneration\n",
    "import torch\n",
    "from itertools import cycle\n",
    "import os \n",
    "from transformers import pipeline\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# For BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate retrieval corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1637/1637 [00:00<00:00, 10191.46it/s]\n"
     ]
    }
   ],
   "source": [
    "path_adam = '/Users/adamwagnerhoegh/Documents/Legal data/domsdatabasen.retsinformation_newer.json'\n",
    "path_asger = \"/Users/asgerkromand/Library/CloudStorage/OneDrive-UniversityofCopenhagen/0. SDS/1 deep learning and nlp/ANLPDP_exam/data/domsdatabasen.retsinformation_newer.json\"\n",
    "path_andreas = '' #missing\n",
    "\n",
    "# Define a function that can cycle through paths the above paths try them out, and yield the path\n",
    "def path():\n",
    "    paths = cycle([path_adam, path_asger, path_andreas])\n",
    "    for path in paths:\n",
    "        if path != '':\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                return data\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            raise FileNotFoundError('No path to data found')\n",
    "\n",
    "retsinfo = path()\n",
    "    \n",
    "rag_list = []\n",
    "idx = 0\n",
    "for lov in tqdm(retsinfo):\n",
    "    for kapitel in lov['kapitler']:\n",
    "        lov_navn = lov['shortName']\n",
    "        for paragraffer in kapitel['paragraffer']:\n",
    "            temp_paragraf_dict = {}\n",
    "            temp_paragraf_dict['paragraf_nr'] = paragraffer['nummer']\n",
    "            temp_paragraf_dict['lovnavn'] = lov_navn\n",
    "            temp_paragraf_list = []\n",
    "            for styk in paragraffer['stk']:\n",
    "                temp_paragraf_list.append(styk['tekst'])\n",
    "            temp_paragraf_dict['text'] = ' '.join(temp_paragraf_list)\n",
    "            rag_list.append(temp_paragraf_dict)\n",
    "\n",
    "with open(\"rag_list.txt\", \"w\") as file:\n",
    "    for item in rag_list:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load excel files in dev set folder\n",
    "import os\n",
    "\n",
    "dev_set_folder = \"devset\"\n",
    "\n",
    "dfs = []\n",
    "for file in os.listdir(dev_set_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(os.path.join(dev_set_folder, file))\n",
    "        dfs.append(df)\n",
    "\n",
    "# merge all excel\n",
    "dev_set = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# add csv\n",
    "rag_batch_1_with_qa = pd.read_csv(\"devset/rag_batch_1_with_qa.csv\", sep=\",\").iloc[:, 1:].dropna()\n",
    "rag_batch_1_with_qa.columns = dev_set.columns\n",
    "dev_set = pd.concat([dev_set, rag_batch_1_with_qa], ignore_index=True)\n",
    "\n",
    "# output dev set\n",
    "dev_set.to_csv(\"devset/dev_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize retrieval corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42593/42593 [00:00<00:00, 114190.09it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_list2 = rag_list\n",
    "\n",
    "def preprocess(rag_list):\n",
    "    # extract and preprocess text\n",
    "    corpus = [item['text'] for item in rag_list]\n",
    "    corpus = [re.sub('\\\\s{2,}', ' ', \n",
    "                     re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                           item.lower())) for item in corpus]\n",
    "\n",
    "    # remove stopwords\n",
    "    #nltk.download('punkt')\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    corpus = [' '.join(word for word in text.split() \n",
    "                      if word not in stop_words) for text in tqdm(corpus)]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = preprocess(rag_list2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval pipeline for BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvis interesser skal foranstaltninger mod interessekonflikter i alternative investeringsfondes indretning beskytte imod? \n",
      "\n",
      "§ 23.: En forvalter af alternative investeringsfonde skal ved indretningen af sin virksomhed organisatorisk såvel som administrativt træffe alle rimelige foranstaltninger med henblik på at identificere, forhindre, styre og overvåge interessekonflikter for at forhindre, at de skader de forvaltede alternative investeringsfondes eller deres investorers interesser. En forvalter skal kunne identificere interessekonflikter, der vil kunne opstå i forbindelse med forvaltningen af alternative investeringsfonde imellem følgende: Forvalteren, herunder dens ledelse, ansatte eller enhver anden person, der direkte eller indirekte er forbundet med forvalteren ved et kontrolforhold, og de forvaltede alternative investeringsfonde eller investorer i sådanne alternative investeringsfonde. Forvalteren, herunder dens ledelse, ansatte eller enhver anden person, der direkte eller indirekte er forbundet med forvalteren ved et kontrolforhold, og de forvaltede alternative investeringsfonde eller investorer i sådanne alternative investeringsfonde. Forskellige forvaltede alternative investeringsfonde eller UCITS indbyrdes eller imellem investorer i sådanne fonde eller UCITS. Forvaltede alternative investeringsfonde eller investorer i sådanne fonde og en eller flere af forvalterens andre kunder. To af forvalterens kunder. Hvis kravene i stk. 1 og 2 ikke med rimelig sikkerhed kan sikre, at risikoen for at skade investorernes interesser vil blive undgået, skal forvalteren klart oplyse investorerne om interessekonflikternes generelle karakter eller kilderne hertil, inden forvalteren påtager sig opgaver på investorernes vegne. Forvalteren skal udarbejde passende politikker og procedurer for håndteringen af interessekonflikter, som skal anvendes, når de organisatoriske ordninger og foranstaltninger efter stk. 1 ikke er tilstrækkelige. Hvis forvalteren anvender prime broker-ydelser på vegne af en alternativ investeringsfond, skal betingelserne for sådanne ydelser være fastsat i en skriftlig aftale. Finanstilsynet kan fastsætte nærmere regler om de foranstaltninger, som forvalteren efter stk. 1 og 3-5 med rimelighed kan forventes at træffe, om de forskellige typer af interessekonflikter, som er omhandlet i stk. 2, og om forebyggelse af interessekonflikter.\n",
      "§ 167.: Finanstilsynet kan påbyde en forvalter af alternative investeringsfonde inden for en af tilsynet fastsat frist at foretage de nødvendige foranstaltninger, hvis forvalterens økonomiske stilling er således forringet, at de alternative investeringsfondes eller investorer i alternative investeringsfondes interesser er udsat for fare, eller forvalterens økonomiske stilling er således forringet, at de alternative investeringsfondes eller investorer i alternative investeringsfondes interesser er udsat for fare, eller der er en ikke uvæsentlig risiko for, at forvalterens økonomiske stilling på grund af indre eller ydre forhold udvikler sig således, at forvalteren vil miste sin tilladelse. Er de påbudte foranstaltninger ikke foretaget inden for den fastsatte frist, kan Finanstilsynet inddrage forvalterens tilladelse.\n",
      "§ 41.: Delegation eller videredelegation af porteføljepleje eller risikostyring må ikke ske til alternative investeringsfondes depositar eller en, der har fået overdraget opgaver fra depositaren, eller alternative investeringsfondes depositar eller en, der har fået overdraget opgaver fra depositaren, eller andre, som kan have interesser, der strider imod forvalteren af alternative investeringsfondes eller den alternative investeringsfonds investorers interesser, medmindre den pågældende foretager porteføljeplejen eller risikostyringen i en særskilt funktion, der er adskilt både funktionelt og hierarkisk fra de funktioner, der varetager andre opgaver, der kunne skabe interessekonflikter, og de potentielle interessekonflikter er fyldestgørende påvist, styret, overvåget og oplyst over for fondens investorer.\n"
     ]
    }
   ],
   "source": [
    "def sparse_retrieval(question, sparse_matrix, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # preprocess and vectorize question\n",
    "    question_processed = [re.sub('\\\\s{2,}', ' ', \n",
    "                               re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                                     question.lower()))]\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_processed = [' '.join(word for word in text.split() \n",
    "                                 if word not in stop_words) for text in question_processed]\n",
    "    \n",
    "    question_vector = vectorizer.transform(question_processed)\n",
    "\n",
    "    # sparse retrieval (cosine similarity)\n",
    "    sparse_retrieval = sparse_matrix.dot(question_vector.T).toarray()\n",
    "\n",
    "    # get top k paragraphs\n",
    "    top_k = np.argsort(sparse_retrieval.flatten())[-k:]\n",
    "\n",
    "    return top_k\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k = sparse_retrieval(random_question, X)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list2[i][\"paragraf_nr\"]}: {rag_list2[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hvornår kan politiet videregive oplysninger om enkeltpersoners rent private forhold til andre myndigheder?\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bm25' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Assuming bm25 is the initialized BM25Okapi model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m top_k_results \u001b[38;5;241m=\u001b[39m bm25_retrieval(random_question, \u001b[43mbm25\u001b[49m, rag_list2, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Print top-k results\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paragraf_nr, text, score \u001b[38;5;129;01min\u001b[39;00m top_k_results:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bm25' is not defined"
     ]
    }
   ],
   "source": [
    "def bm25_retrieval(question, bm25_model, rag_list, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of the most relevant paragraphs based on BM25.\n",
    "    \"\"\"\n",
    "    # Preprocess and tokenize the question\n",
    "    question_processed = re.sub(r'\\s{2,}', ' ', \n",
    "                                 re.sub(r'\\W|[0-9]|§', ' ', question.lower()))\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_tokens = [word for word in question_processed.split() if word not in stop_words]\n",
    "\n",
    "    # Get BM25 scores for the query\n",
    "    scores = bm25_model.get_scores(question_tokens)\n",
    "\n",
    "    # Get the top k results\n",
    "    top_k_indices = np.argsort(scores)[-k:][::-1]  # Sort scores in descending order\n",
    "\n",
    "    # Return the top k paragraphs\n",
    "    return [(rag_list[i]['paragraf_nr'], rag_list[i]['text'], scores[i]) for i in top_k_indices]\n",
    "\n",
    "# Example Usage\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(f\"Question: {random_question}\\n\")\n",
    "\n",
    "# Assuming bm25 is the initialized BM25Okapi model\n",
    "top_k_results = bm25_retrieval(random_question, bm25, rag_list2, k=3)\n",
    "\n",
    "# Print top-k results\n",
    "for paragraf_nr, text, score in top_k_results:\n",
    "    print(f\"{paragraf_nr}: {text} (Score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embedding_matrix(pooling, save=True, save_folder=None):\n",
    "    # initialise model\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "    bert_model = AutoModel.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "\n",
    "    # define device\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # move model to device\n",
    "    bert_model.to(device)\n",
    "\n",
    "    # create list of embedding vectors to concatenate into a torch tensor\n",
    "    embeddings = []\n",
    "\n",
    "    # index to track numer of mistakes\n",
    "    idx = 0\n",
    "\n",
    "    for item in tqdm(rag_list):\n",
    "        # doing a try and except as some paragraphs may exceed the context window of the BERT (I believe)\n",
    "        try:\n",
    "            # tokenize texts\n",
    "            input_ids = bert_tokenizer.encode(item['text'], return_tensors='pt').to(device)\n",
    "            # run through BERT\n",
    "            with torch.no_grad():  # disable gradient computation for inference\n",
    "                outputs = bert_model(input_ids)\n",
    "            \n",
    "            # different kinds of pooling\n",
    "            if pooling == 'cls':\n",
    "                embedding_vector = outputs.last_hidden_state[:, 0, :]\n",
    "            elif pooling == 'max':\n",
    "                embedding_vector = torch.max(outputs, dim=1)[0]\n",
    "            elif pooling == 'mean':\n",
    "                embedding_vector = torch.mean(outputs, dim=1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling method: {pooling}\")\n",
    "            \n",
    "            # add cls-vector to list of embeddings\n",
    "            embeddings.append(embedding_vector)\n",
    "        except:\n",
    "            # if error then count errors with this\n",
    "            idx += 1\n",
    "\n",
    "    print(f'{idx} no. of errors')\n",
    "\n",
    "    # concatenate list into torch tensor\n",
    "    embeddings_tensor = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    if save == True:\n",
    "        # make sure that folder exists\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "        # save tensor \n",
    "        torch.save(embeddings_tensor, f'{save_folder}/{pooling}_embeddings_tensor.pt')\n",
    "\n",
    "    return embeddings_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_embedding_matrix(pooling='cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at vesteinn/DanskBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "bert_model = AutoModel.from_pretrained(\"vesteinn/DanskBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = torch.load('/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_DanskBERT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvornår kommer de i loven for livs-, ulykkes- og sygeforsikring givne regler om begunstigede til anvendelse? \n",
      "\n",
      "Hvis en virksomheds tilsvar af skatter og afgifter m.v., der opkræves efter reglerne i denne lov, for en afregningsperiode er negativt, udbetales beløbet til virksomheden. Såfremt angivelsen henholdsvis indberetningen af beløb omfattet af § 2, stk. 1, 4. pkt. er modtaget rettidigt, sker udbetaling efter stk. 1 senest 3 uger efter modtagelsen af angivelsen henholdsvis indberetningen for den pågældende periode. Kan told- og skatteforvaltningen på grund af virksomhedens forhold ikke foretage kontrol af angivelsen eller indberetningen af beløb omfattet af § 2, stk. 1, 4. pkt., afbrydes udbetalingsfristen, indtil virksomhedens forhold ikke længere hindrer kontrol. Beløb, der skulle have været udbetalt efter stk. 1, kan tilbageholdes, såfremt angivelser eller indberetningen af beløb omfattet af § 2, stk. 1, 4. pkt. vedrørende afsluttede afregningsperioder ikke er indgivet. Negative tilsvar efter stk. 1, der indgår ved en samlet kontoopgørelse af virksomhedens tilsvar af skatter og afgifter m.v. efter reglerne i denne lovs kapitel 5, kan alene udbetales, hvis det negative tilsvar modsvares af en kreditsaldo opgjort efter § 16 a, stk. 2, 2. pkt.\n",
      "For virksomheder, der i medfør af momslovens § 57, stk. 4, anvender kalenderhalvåret som afgiftsperiode i første og andet halvår af 2020, sammenlægges første halvdel af kalenderåret 2020 med anden halvdel af kalenderåret 2020, således at angivelsesfristen for den samlede periode udløber den 1. marts 2021, medmindre virksomheden angiver et momstilsvar for første halvår 2020 senest den 1. september 2020.\n",
      "Som grundlag for kommunalbestyrelsens afgørelser efter §§ 125 og 128, § 128 b, stk. 2, §§ 128 c-128 f og § 136 a, stk. 3, skal der foreligge oplysninger om den socialpædagogiske hjælp og pleje efter kapitel 16, der har været iværksat før den påtænkte afgørelse om iværksættelse af foranstaltningerne, oplysninger om den socialpædagogiske hjælp og pleje efter kapitel 16, der har været iværksat før den påtænkte afgørelse om iværksættelse af foranstaltningerne, oplysninger om den forventede periode, i hvilken foranstaltningerne vil være nødvendige, og pårørendes og en eventuel værges bemærkninger til de påtænkte foranstaltninger.\n"
     ]
    }
   ],
   "source": [
    "def dense_retrieval(question, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the input sentence\n",
    "    input_ids = bert_tokenizer.encode(question, return_tensors=\"pt\")  # Encode and add batch dimension\n",
    "    # Pass the input through the model\n",
    "    \n",
    "    with torch.no_grad():  # disable gradient computation for inference\n",
    "        outputs = bert_model(input_ids)\n",
    "\n",
    "    # Extract the CLS token representation\n",
    "    cls_vector = outputs.last_hidden_state[:, 0, :]  # CLS token is at position 0\n",
    "    \n",
    "    # sparse retrieval (cosine similarity)\n",
    "    dense_retrieval = embeddings_matrix @ torch.transpose(cls_vector, 0, 1)\n",
    "    \n",
    "    # get top k paragraphs\n",
    "    top_k_indices = torch.sort(dense_retrieval, descending=True, dim=0)[1][:k]\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k = dense_retrieval(random_question, k=3)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name, retrieval_method, metric, k):\n",
    "    \"\"\"\n",
    "    model_name = 'KennethTM/gpt-neo-1.3B-danish' or 'strombergnlp/dant5-large'\n",
    "    retrieval_method = 'tf-idf', 'bm25' or 'dense'\n",
    "    metric = 'bleu', 'rouge' or 'meteor'\n",
    "    \"\"\"\n",
    "    # set device to mps\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # load AutoTokenizer for model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # generate list of answers to fill\n",
    "    answers = []\n",
    "\n",
    "    # load neo\n",
    "    # the loops are made for each model to not waste compute on loading models for each question\n",
    "\n",
    "    if model_name == 'KennethTM/gpt-neo-1.3B-danish':\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"KennethTM/gpt-neo-1.3B-danish\").to(device)\n",
    "    \n",
    "        for question in tqdm(dev_set['question, str'], desc='Answering questions with neo'):\n",
    "\n",
    "            if retrieval_method == 'tf-idf':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in sparse_retrieval(question, X, k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            elif retrieval_method == 'bm25':\n",
    "                paragraphs = bm25_retrieval(rag_list=rag_list, k=3)\n",
    "            \n",
    "            elif retrieval_method == 'dense_retrieval':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in dense_retrieval(question, k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            # assemble all in prompt\n",
    "            prompt = f'Kontekst: {paragraphs} Spørgsmål: {question} Svar: '\n",
    "\n",
    "            # tokenize\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # set max_length to no. of tokens in prompt + 100 (the 100 are thus for generation)\n",
    "            max_length = int(input_ids['input_ids'].size(1)) + 100\n",
    "\n",
    "            # generate answer with no_grad() to save compute\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=max_length,\n",
    "                    pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            # decode the generated answer\n",
    "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            answers.append(answer)\n",
    "    \n",
    "\n",
    "    # load T5\n",
    "    elif model_name == 'strombergnlp/dant5-large':\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"strombergnlp/dant5-large\").to(device)\n",
    "\n",
    "        for question in tqdm(dev_set['question, str'], desc='Answering questions with T5'):\n",
    "\n",
    "            if retrieval_method == 'tf-idf':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in sparse_retrieval(question, X, k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            elif retrieval_method == 'bm25':\n",
    "                paragraphs = bm25_retrieval(rag_list=rag_list, k=3)\n",
    "            \n",
    "            elif retrieval_method == 'dense_retrieval':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in dense_retrieval(question, k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            # assemble all in input\n",
    "            input_text = f\"Spørgsmål: {question} Kontekst: {paragraphs} Svar:\"\n",
    "\n",
    "            # tokenize\n",
    "            input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "            # generate answer with no_grad() to save compute\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=max_length,\n",
    "                    pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(input_ids, max_length=100)\n",
    "\n",
    "            # Decode and print the generated answer\n",
    "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            answers.append(answer)\n",
    "        \n",
    "    # choosing metric to evaluate answers\n",
    "    if metric == 'bleu':\n",
    "        scores = []\n",
    "        idx = 0\n",
    "\n",
    "        for pred_answer, true_answer in tqdm(zip(answers, list(dev_set['answer, str'])), desc='Evaluating with bleu'):\n",
    "            try:\n",
    "                scores.append(nltk.translate.bleu_score.sentence_bleu([true_answer], pred_answer))\n",
    "            except:\n",
    "                print(f'Error when computing bleu-score at index {idx}')\n",
    "            idx += 1\n",
    "    \n",
    "    elif metric == 'meteor':\n",
    "        scores = []\n",
    "        idx = 0\n",
    "\n",
    "        for pred_answer, true_answer in tqdm(zip(answers, list(dev_set['answer, str'])), desc='Evaluating with meteor'):\n",
    "            try:\n",
    "                scores.append(nltk.tranlsate.meteor_score([true_answer], pred_answer))\n",
    "            except:\n",
    "                print(f'Error when computing meteor-score at index {idx}')\n",
    "            idx += 1\n",
    "\n",
    "    elif metric == 'rouge':\n",
    "        scores = []\n",
    "        idx = 0\n",
    "\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "        for pred_answer, true_answer in tqdm(zip(answers, list(dev_set['answer, str'])), desc='Evaluating with rouge'):\n",
    "            try:\n",
    "                scores.append(score.score(true_answer, pred_answer)['rouge1'])\n",
    "            except:\n",
    "                print(f'Error when computing meteor-score at index {idx}')\n",
    "            idx += 1\n",
    "\n",
    "    print(f'{metric}-scores for {model_name} using {retrieval_method}: {np.mean(scores)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
