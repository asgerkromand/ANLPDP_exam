{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/adamwagnerhoegh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, T5ForConditionalGeneration\n",
    "import torch\n",
    "from itertools import cycle\n",
    "import os \n",
    "from transformers import pipeline\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# For BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate retrieval corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1637/1637 [00:00<00:00, 9619.84it/s]\n"
     ]
    }
   ],
   "source": [
    "path_adam = '/Users/adamwagnerhoegh/Documents/Legal data/domsdatabasen.retsinformation_newer.json'\n",
    "path_asger = \"/Users/asgerkromand/Library/CloudStorage/OneDrive-UniversityofCopenhagen/0. SDS/1 deep learning and nlp/ANLPDP_exam/data/domsdatabasen.retsinformation_newer.json\"\n",
    "path_andreas = '' #missing\n",
    "\n",
    "# Define a function that can cycle through paths the above paths try them out, and yield the path\n",
    "def path():\n",
    "    paths = cycle([path_adam, path_asger, path_andreas])\n",
    "    for path in paths:\n",
    "        if path != '':\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                return data\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            raise FileNotFoundError('No path to data found')\n",
    "\n",
    "retsinfo = path()\n",
    "    \n",
    "rag_list = []\n",
    "idx = 0\n",
    "for lov in tqdm(retsinfo):\n",
    "    for kapitel in lov['kapitler']:\n",
    "        lov_navn = lov['shortName']\n",
    "        for paragraffer in kapitel['paragraffer']:\n",
    "            temp_paragraf_dict = {}\n",
    "            temp_paragraf_dict['paragraf_nr'] = paragraffer['nummer']\n",
    "            temp_paragraf_dict['lovnavn'] = lov_navn\n",
    "            temp_paragraf_list = []\n",
    "            for styk in paragraffer['stk']:\n",
    "                temp_paragraf_list.append(styk['tekst'])\n",
    "            temp_paragraf_dict['text'] = ' '.join(temp_paragraf_list)\n",
    "            rag_list.append(temp_paragraf_dict)\n",
    "\n",
    "with open(\"rag_list.txt\", \"w\") as file:\n",
    "    for item in rag_list:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load excel files in dev set folder\n",
    "import os\n",
    "\n",
    "dev_set_folder = \"devset\"\n",
    "\n",
    "dfs = []\n",
    "for file in os.listdir(dev_set_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(os.path.join(dev_set_folder, file))\n",
    "        dfs.append(df)\n",
    "\n",
    "# merge all excel\n",
    "dev_set = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# add csv\n",
    "rag_batch_1_with_qa = pd.read_csv(\"devset/rag_batch_1_with_qa.csv\", sep=\",\").iloc[:, 1:].dropna()\n",
    "rag_batch_1_with_qa.columns = dev_set.columns\n",
    "dev_set = pd.concat([dev_set, rag_batch_1_with_qa], ignore_index=True)\n",
    "\n",
    "# output dev set\n",
    "dev_set.to_csv(\"devset/dev_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize retrieval corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42593/42593 [00:00<00:00, 112679.32it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_list2 = rag_list\n",
    "\n",
    "def preprocess(rag_list):\n",
    "    # extract and preprocess text\n",
    "    corpus = [item['text'] for item in rag_list]\n",
    "    corpus = [re.sub('\\\\s{2,}', ' ', \n",
    "                     re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                           item.lower())) for item in corpus]\n",
    "\n",
    "    # remove stopwords\n",
    "    #nltk.download('punkt')\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    corpus = [' '.join(word for word in text.split() \n",
    "                      if word not in stop_words) for text in tqdm(corpus)]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = preprocess(rag_list2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval pipeline for BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvilken minister kan fastsætte regler om naturbeskyttelse på dansk territorium vedrørende transport af olie i rørledninger? \n",
      "\n",
      "§ 1 a.: Klima-, energi- og forsyningsministeren kan fastsætte regler med henblik på at gennemføre eller anvende internationale konventioner og EU-regler om forhold, der er omfattet af denne lov, herunder forordninger, direktiver og beslutninger om naturbeskyttelse på dansk kontinentalsokkel, i dansk eksklusiv økonomisk zone og på dansk søterritorium.\n",
      "§ 2 a.: Klima-, energi- og forsyningsministeren kan meddele tilladelse til forundersøgelser med henblik på nedlæggelse af nye elkabler eller rørledninger til transport af kulbrinter eller ændringer af eksisterende elkabler eller rørledninger til transport af kulbrinter på dansk kontinentalsokkelområde og på dansk søterritorium. Klima-, energi- og forsyningsministeren kan fastsætte vilkår for tilladelsen efter stk. 1, herunder om de forhold, der skal undersøges, om forundersøgelsernes forløb og tidsrum og om overholdelse af miljø- og sikkerhedskrav. Klima-, energi- og forsyningsministeren kan pålægge ansøgere efter stk. 1 at meddele alle oplysninger, der er nødvendige for behandlingen af ansøgningen og for tilsynet med tilladelsen. Klima-, energi- og forsyningsministeren kan fastsætte nærmere regler om krav til ansøgninger og tilladelser efter stk. 1.\n",
      "§ 18.: Konvertering af rørledninger, der tidligere har været anvendt til transport af olie eller gas, til rørledningsanlæg til transport af CO Klima-, energi- og forsyningsministeren kan fastsætte regler om konvertering af rørledninger, der tidligere har været anvendt til transport af olie eller gas, til rørledningsanlæg til transport af CO\n"
     ]
    }
   ],
   "source": [
    "def sparse_retrieval(question, sparse_matrix, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # preprocess and vectorize question\n",
    "    question_processed = [re.sub('\\\\s{2,}', ' ', \n",
    "                               re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                                     question.lower()))]\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_processed = [' '.join(word for word in text.split() \n",
    "                                 if word not in stop_words) for text in question_processed]\n",
    "    \n",
    "    question_vector = vectorizer.transform(question_processed)\n",
    "\n",
    "    # sparse retrieval (cosine similarity)\n",
    "    sparse_retrieval = sparse_matrix.dot(question_vector.T).toarray()\n",
    "\n",
    "    # get top k paragraphs\n",
    "    top_k = np.argsort(sparse_retrieval.flatten())[-k:]\n",
    "\n",
    "    return top_k\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k = sparse_retrieval(random_question, X)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list2[i][\"paragraf_nr\"]}: {rag_list2[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hvad er den mindste selskabskapital, et anpartsselskab skal have?\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bm25' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Assuming bm25 is the initialized BM25Okapi model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m top_k_results \u001b[38;5;241m=\u001b[39m bm25_retrieval(random_question, \u001b[43mbm25\u001b[49m, rag_list2, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Print top-k results\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paragraf_nr, text, score \u001b[38;5;129;01min\u001b[39;00m top_k_results:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bm25' is not defined"
     ]
    }
   ],
   "source": [
    "def bm25_retrieval(question, bm25_model, rag_list, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of the most relevant paragraphs based on BM25.\n",
    "    \"\"\"\n",
    "    # Preprocess and tokenize the question\n",
    "    question_processed = re.sub(r'\\s{2,}', ' ', \n",
    "                                 re.sub(r'\\W|[0-9]|§', ' ', question.lower()))\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_tokens = [word for word in question_processed.split() if word not in stop_words]\n",
    "\n",
    "    # Get BM25 scores for the query\n",
    "    scores = bm25_model.get_scores(question_tokens)\n",
    "\n",
    "    # Get the top k results\n",
    "    top_k_indices = np.argsort(scores)[-k:][::-1]  # Sort scores in descending order\n",
    "\n",
    "    # Return the top k paragraphs\n",
    "    return [(rag_list[i]['paragraf_nr'], rag_list[i]['text'], scores[i]) for i in top_k_indices]\n",
    "\n",
    "# Example Usage\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(f\"Question: {random_question}\\n\")\n",
    "\n",
    "# Assuming bm25 is the initialized BM25Okapi model\n",
    "top_k_results = bm25_retrieval(random_question, bm25, rag_list2, k=3)\n",
    "\n",
    "# Print top-k results\n",
    "for paragraf_nr, text, score in top_k_results:\n",
    "    print(f\"{paragraf_nr}: {text} (Score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embedding_matrix(pooling, save=False, save_folder=None):\n",
    "    # initialise model\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "    bert_model = AutoModel.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "\n",
    "    # define device\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # move model to device\n",
    "    bert_model.to(device)\n",
    "\n",
    "    # create list of embedding vectors to concatenate into a torch tensor\n",
    "    embeddings = []\n",
    "\n",
    "    # index to track numer of mistakes\n",
    "    idx = 0\n",
    "\n",
    "    for item in tqdm(rag_list):\n",
    "        # doing a try and except as some paragraphs may exceed the context window of the BERT (I believe)\n",
    "        try:\n",
    "            # tokenize texts\n",
    "            input_ids = bert_tokenizer.encode(item['text'], return_tensors='pt').to(device)\n",
    "            # run through BERT\n",
    "            with torch.no_grad():  # disable gradient computation for inference\n",
    "                outputs = bert_model(input_ids)\n",
    "            \n",
    "            # different kinds of pooling\n",
    "            if pooling == 'cls':\n",
    "                embedding_vector = outputs.last_hidden_state[:, 0, :]\n",
    "            elif pooling == 'max':\n",
    "                embedding_vector = torch.max(outputs, dim=1)[0]\n",
    "            elif pooling == 'mean':\n",
    "                embedding_vector = torch.mean(outputs, dim=1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling method: {pooling}\")\n",
    "            \n",
    "            # add cls-vector to list of embeddings\n",
    "            embeddings.append(embedding_vector)\n",
    "        except:\n",
    "            # if error then count errors with this\n",
    "            embeddings.append(torch.zeros(1, 768))\n",
    "            idx += 1\n",
    "\n",
    "    print(f'{idx} no. of errors')\n",
    "\n",
    "    # concatenate list into torch tensor\n",
    "    embeddings_tensor = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    if save == True:\n",
    "        # make sure that folder exists\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "        # save tensor \n",
    "        torch.save(embeddings_tensor, f'{save_folder}/{pooling}_embeddings_tensor.pt')\n",
    "\n",
    "    return embeddings_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at KennethTM/bert-base-uncased-danish and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 87/42593 [00:06<51:36, 13.73it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
      "  1%|          | 454/42593 [00:41<1:24:13,  8.34it/s]"
     ]
    }
   ],
   "source": [
    "#create_embedding_matrix(pooling='cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at vesteinn/DanskBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "bert_model = AutoModel.from_pretrained(\"vesteinn/DanskBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can load cls or max respectively, mean still needs to be created\n",
    "embeddings_matrix = torch.load('/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_DanskBERT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bedriftværn, der er etableret i henhold til den lovgivning, der er gældende indtil 1. januar 1993, opretholdes, medmindre at hvad? \n",
      "\n",
      "Uddannelsesparate og aktivitetsparate personer under 30 år uden en erhvervskompetencegivende uddannelse modtager uddannelseshjælp. Uddannelseshjælpen udgør et månedligt beløb på 11.505 kr. for personer, der forsørger eget barn i hjemmet og har erhvervet ret til ekstra børnetilskud efter lov om børnetilskud og forskudsvis udbetaling af børnebidrag, jf. dog stk. 5, 11.505 kr. for personer, der forsørger eget barn i hjemmet og har erhvervet ret til ekstra børnetilskud efter lov om børnetilskud og forskudsvis udbetaling af børnebidrag, jf. dog stk. 5, 8.051 kr. for personer, der forsørger eget barn i hjemmet og ikke har erhvervet ret til ekstra børnetilskud efter lov om børnetilskud og forskudsvis udbetaling af børnebidrag, 10.500 kr. for kvinder, der er gravide og har passeret 12. svangerskabsuge, 13.952 kr. for personer, der har en dokumenteret psykisk lidelse, der er diagnosticeret som skizofreni, skizotypisk sindslidelse, vedvarende psykotisk tilstand, korterevarende psykotisk tilstand, skizoaffektiv lidelse, uspecificeret ikke organisk betinget psykose og emotionelt ustabil personlighedsstruktur af borderlinetype, og har forsørgelsespligt over for børn, 10.500 kr. for personer, der ikke bor hos en eller begge forældre og har en dokumenteret psykisk lidelse som anført i nr. 4, 5.753 kr. for personer, der er fyldt 25 år og ikke bor hos en eller begge forældre, 2.479 kr. for personer, der er fyldt 25 år og bor hos en eller begge forældre, 5.753 kr. for personer under 25 år, der ikke bor hos en eller begge forældre, og 2.479 kr. for personer under 25 år, der bor hos en eller begge forældre. En person, der har en dokumenteret bidragspligt over for et barn, og som modtager uddannelseshjælp efter stk. 2, nr. 6, 7, 8 eller 9, får et månedligt tillæg, der svarer til det fastsatte bidrag, dog højst normalbidraget. Hjælp efter stk. 2, nr. 4, og stk. 3 er betinget af, at børnene opholder sig her i landet. For en enlig forsørger, der ikke har erhvervet ret til ekstra børnetilskud som følge af betingelserne i § 5, stk. 1, nr. 1, eller § 5 a i lov om børnetilskud og forskudsvis udbetaling af børnebidrag, ydes uddannelseshjælpen med den sats, som den enlige forsørger ville have haft ret til, hvis betingelserne i lov om børnetilskud og forskudsvis udbetaling af børnebidrag var opfyldt.\n",
      "Med patientens samtykke kan sundhedspersoner videregive oplysninger til andre sundhedspersoner om patientens helbredsforhold og andre fortrolige oplysninger i forbindelse med behandling af patienten eller behandling af andre patienter. Videregivelse af de i stk. 1 nævnte oplysninger kan uden patientens samtykke ske, når det er nødvendigt af hensyn til et aktuelt behandlingsforløb for patienten, og videregivelsen sker under hensyntagen til patientens interesse og behov, det er nødvendigt af hensyn til et aktuelt behandlingsforløb for patienten, og videregivelsen sker under hensyntagen til patientens interesse og behov, videregivelsen omfatter et udskrivningsbrev fra en læge, der er ansat i sygehusvæsenet, til patientens alment praktiserende læge eller den praktiserende speciallæge, der har henvist patienten til sygehusbehandling, videregivelsen omfatter et udskrivningsbrev fra en læge, der er ansat på privatejet sygehus, klinik m.v., til de i nr. 2 nævnte læger, når behandlingen er ydet efter aftale med et regionsråd eller en kommunalbestyrelse i henhold til denne lov, videregivelsen er nødvendig til berettiget varetagelse af en åbenbar almen interesse eller af væsentlige hensyn til patienten, herunder en patient, der ikke selv kan varetage sine interesser, sundhedspersonen eller andre, videregivelsen sker til patientens alment praktiserende læge fra en læge, der virker som stedfortræder for denne, videregivelsen sker til en læge, tandlæge, jordemoder eller sygeplejerske om en patient, som modtageren tidligere har deltaget i behandlingen af, når videregivelsen er nødvendig og relevant til brug for evaluering af modtagerens egen indsats i behandlingen eller som dokumentation for erhvervede kvalifikationer i et uddannelsesforløb og videregivelsen er nødvendig og relevant til brug for evaluering af modtagerens egen indsats i behandlingen eller som dokumentation for erhvervede kvalifikationer i et uddannelsesforløb og videregivelsen sker under hensyntagen til patientens interesse og behov, eller videregivelsen sker til en studerende, der som led i en sundhedsvidenskabelig eller sundhedsfaglig uddannelse deltager i behandlingen af en patient uden at være medhjælp, når videregivelsen er nødvendig for den studerendes forståelse af behandlingssituationen eller evaluering af den studerendes deltagelse i behandlingssituationen og videregivelsen er nødvendig for den studerendes forståelse af behandlingssituationen eller evaluering af den studerendes deltagelse i behandlingssituationen og videregivelsen sker under hensyntagen til patientens interesse og behov. Patienten kan frabede sig, at oplysninger efter stk. 2, nr. 1-3, 6 og 7, videregives. Den sundhedsperson, der er i besiddelse af en fortrolig oplysning, afgør, hvorvidt videregivelse efter stk. 2 er berettiget. Såfremt der manuelt videregives oplysninger efter stk. 2, nr. 4, skal den, oplysningen angår, snarest muligt herefter orienteres om videregivelsen og formålet hermed, medmindre orientering kan udelades efter anden lovgivning eller af hensyn til offentlige eller private interesser svarende til dem, der beskyttes i denne lovgivning. Videregivelse efter stk. 2, nr. 6, må kun ske i umiddelbar forlængelse af behandlingsforløbet og senest 6 måneder efter den anmodende læges, tandlæges, jordemoders eller sygeplejerskes afslutning af behandlingen eller viderehenvisning af patienten, medmindre videregivelsen er påkrævet som led i speciallæge- eller specialtandlægeuddannelsen. En vejleder for en læge eller tandlæge under uddannelse til speciallæge eller -tandlæge har samme adgang til helbredsoplysninger m.v. efter stk. 2, nr. 6, som lægen eller tandlægen under uddannelse. Indenrigs- og sundhedsministeren fastsætter nærmere regler om videregivelse af helbredsoplysninger m.v. efter denne bestemmelse, herunder om videregivelsens omfang og om gennemførelsen heraf.\n",
      "Børne- og undervisningsministeren yder tilskud eller lån til institutionerne efter stk. 2-9. Børne- og undervisningsministeren yder tilskud til fællesudgifter i form af grundtilskud, der fastsættes på de årlige finanslove, og taxametertilskud, der ydes ud fra den enkelte institutions antal årselever og en takst pr. årselev. Børne- og undervisningsministeren yder taxametertilskud til institutionernes erhvervelse og opretholdelse af lokaler, bygninger og arealer ud fra den enkelte institutions antal årselever og en takst pr. årselev. Børne- og undervisningsministeren fastsætter regler om opgørelse af årselever efter stk. 2 og 3. Udenlandske deltagere i erhvervsuddannelser kan kun indgå i beregningen efter stk. 2 og 3 og § 19, stk. 4, hvis de er meddelt opholdstilladelse med henblik på midlertidigt ophold, jf. udlændingelovens § 7, stk. 1-3, § 8, stk. 1 eller 2, eller § 9 c, stk. 3, nr. 1 eller 2, tidsubegrænset opholdstilladelse eller tidsbegrænset opholdstilladelse med mulighed for varigt ophold i Danmark, er meddelt opholdstilladelse med henblik på midlertidigt ophold, jf. udlændingelovens § 7, stk. 1-3, § 8, stk. 1 eller 2, eller § 9 c, stk. 3, nr. 1 eller 2, tidsubegrænset opholdstilladelse eller tidsbegrænset opholdstilladelse med mulighed for varigt ophold i Danmark, er meddelt tidsbegrænset opholdstilladelse med henblik på midlertidigt ophold efter udlændingelovens § 9, stk. 1, eller § 9 c, stk. 1, som følge af en familiemæssig tilknytning til en udlænding, der er meddelt opholdstilladelse efter udlændingelovens § 7, stk. 1-3, eller § 8, stk. 1 eller 2, er meddelt opholdstilladelse efter udlændingelovens § 9 m, som medfølgende barn af en udlænding, som dels er statsborger i et land, der ikke er tilsluttet Den Europæiske Union eller omfattet af EØS-aftalen, dels er meddelt opholdstilladelse efter udlændingelovens § 9 a, jf. dog stk. 6, er meddelt opholdstilladelse efter lov om midlertidig opholdstilladelse til personer, der i Afghanistan har bistået danske myndigheder m.v., Lov om midlertidig opholdstilladelse til personer, der i Afghanistan har bistået danske myndigheder m.v. ophæves den 1. december 2025, jf. § 28, stk. 1, i lov nr. 2055 af 16. november 2021, som ændret ved lov nr. 1334 af 25. november 2023, medmindre andet bestemmes ved lov. er meddelt opholdstilladelse efter lov om midlertidig opholdstilladelse til personer, der er fordrevet fra Ukraine, er udvekslet med danske deltagere efter aftale mellem institutionen og en institutionen i udlandet eller efter EU-retten, herunder EØS-aftalen, eller internationale aftaler, som Danmark har indgået, har krav på ligestilling med danske statsborgere. En udenlandsk deltager er omfattet af stk. 5, nr. 3, uanset at forælderens opholdstilladelse efter udlændingelovens § 9 a ophører efter tidspunktet for påbegyndelsen af uddannelsen. Institutionens udbud af erhvervsuddannelser til andre udenlandske deltagere end dem, der er nævnt i stk. 5 og 6, sker som indtægtsdækket virksomhed. Børne- og undervisningsministeren kan yde lån og tilskud til institutioner, der efter ministerens skøn er kommet i en særlig vanskelig økonomisk situation. Børne- og undervisningsministeren kan yde tilskud og lån i forbindelse med sammenlægning eller spaltning af institutioner. Uanset stk. 2 og 3 og § 19, stk. 1 og 2, kan der på de årlige finanslove fastsættes særskilte takster for fjernundervisning, herunder at der ved fjernundervisning ikke ydes tilskud til dækning af institutionernes erhvervelse og opretholdelse af lokaler, bygninger og arealer. Udbetaling Danmark varetager administrative opgaver af finansiel og regnskabsmæssig karakter vedrørende lån efter stk. 8 og 9.\n"
     ]
    }
   ],
   "source": [
    "def dense_retrieval(question, pooling='cls', k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    pooling = 'cls', 'max' or 'mean'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the input sentence\n",
    "    input_ids = bert_tokenizer.encode(question, return_tensors=\"pt\")  # Encode and add batch dimension\n",
    "    # Pass the input through the model\n",
    "    \n",
    "    with torch.no_grad():  # disable gradient computation for inference\n",
    "        outputs = bert_model(input_ids)\n",
    "    \n",
    "    if pooling == 'cls':\n",
    "        # Extract the CLS token representation\n",
    "        embedding_vector = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    elif pooling == 'max':\n",
    "        embedding_vector = torch.max(outputs.last_hidden_state, dim=1)[0]\n",
    "\n",
    "    elif pooling == 'mean':\n",
    "        embedding_vector = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    \n",
    "    # normalise the cls-embedding and the embedding matrix so that the dot product\n",
    "    # below is now cosine similarity\n",
    "    embedding_vector_normalised = embedding_vector / torch.norm(embedding_vector, dim=-1, keepdim=True)\n",
    "    embeddings_matrix_normalised = embeddings_matrix / torch.norm(embeddings_matrix, dim=-1, keepdim=True)\n",
    "\n",
    "    # finding most similar vectors with dot product\n",
    "    dense_retrieval = embeddings_matrix_normalised @ torch.transpose(embedding_vector_normalised, 0, 1)\n",
    "    \n",
    "    # get top k paragraphs\n",
    "    top_k_indices = torch.sort(dense_retrieval, descending=True, dim=0)[1][:k]\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n') \n",
    "top_k = dense_retrieval(random_question, pooling='cls', k=3)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF, k=1: 100%|██████████| 106/106 [00:00<00:00, 296.90it/s]\n",
      "TF-IDF, k=3: 100%|██████████| 106/106 [00:00<00:00, 319.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# add retrieved paragraphs to dev_set\n",
    "\n",
    "tf_idf_1 = []\n",
    "\n",
    "for question in tqdm(dev_set['question, str'], desc='TF-IDF, k=1'):\n",
    "    paragraphs = [rag_list[i]['text'] for i in sparse_retrieval(question, X, k=1)]\n",
    "    # join list into long string\n",
    "    paragraphs = ' '.join(paragraphs)\n",
    "    tf_idf_1.append(paragraphs)\n",
    "\n",
    "\n",
    "tf_idf_3 = []\n",
    "\n",
    "for question in tqdm(dev_set['question, str'], desc='TF-IDF, k=3'):\n",
    "    paragraphs = [rag_list[i]['text'] for i in sparse_retrieval(question, X, k=3)]\n",
    "    # join list into long string\n",
    "    paragraphs = ' '.join(paragraphs)\n",
    "    tf_idf_3.append(paragraphs)\n",
    "\n",
    "\n",
    "dev_set['tf_idf_1'] = tf_idf_1\n",
    "dev_set['tf_idf_3'] = tf_idf_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name, retrieval_method, metric, k):\n",
    "    \"\"\"\n",
    "    model_name = 'KennethTM/gpt-neo-1.3B-danish' or 'strombergnlp/dant5-large'\n",
    "    retrieval_method = 'tf-idf', 'bm25' or 'dense'\n",
    "    metric = 'bleu', 'rouge' or 'meteor'\n",
    "    \"\"\"\n",
    "    # set device to mps\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # load AutoTokenizer for model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # generate list of answers to fill\n",
    "    answers = []\n",
    "\n",
    "    # load neo\n",
    "    # the loops are made for each model to not waste compute on loading models for each question\n",
    "\n",
    "    if model_name == 'KennethTM/gpt-neo-1.3B-danish':\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"KennethTM/gpt-neo-1.3B-danish\").to(device)\n",
    "    \n",
    "        for question in tqdm(dev_set['question, str'], desc='Answering questions with neo'):\n",
    "\n",
    "            if retrieval_method == 'tf-idf':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in sparse_retrieval(question, X, k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            elif retrieval_method == 'bm25':\n",
    "                paragraphs = bm25_retrieval(rag_list=rag_list, k=3)\n",
    "            \n",
    "            elif retrieval_method == 'dense_retrieval':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in dense_retrieval(question, k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            # assemble all in prompt\n",
    "            prompt = f'Kontekst: {paragraphs} Spørgsmål: {question} Svar: '\n",
    "\n",
    "            # tokenize\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # set max_length to no. of tokens in prompt + 100 (the 100 are thus for generation)\n",
    "            max_length = int(input_ids['input_ids'].size(1)) + 100\n",
    "\n",
    "            # generate answer with no_grad() to save compute\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=max_length,\n",
    "                    pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            # decode the generated answer\n",
    "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            answers.append(answer)\n",
    "    \n",
    "\n",
    "    # load T5\n",
    "    elif model_name == 'strombergnlp/dant5-large':\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"strombergnlp/dant5-large\").to(device)\n",
    "\n",
    "        for question in tqdm(dev_set['question, str'], desc='Answering questions with T5'):\n",
    "\n",
    "            if retrieval_method == 'tf-idf':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in sparse_retrieval(question, X, k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            elif retrieval_method == 'bm25':\n",
    "                paragraphs = bm25_retrieval(rag_list=rag_list, k=3)\n",
    "            \n",
    "            elif retrieval_method == 'dense_retrieval':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in dense_retrieval(question, k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            # assemble all in input\n",
    "            input_text = f\"Spørgsmål: {question} Kontekst: {paragraphs} Svar:\"\n",
    "\n",
    "            # tokenize\n",
    "            input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "            # generate answer with no_grad() to save compute\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=max_length,\n",
    "                    pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(input_ids, max_length=100)\n",
    "\n",
    "            # Decode and print the generated answer\n",
    "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            answers.append(answer)\n",
    "        \n",
    "    # choosing metric to evaluate answers\n",
    "    if metric == 'bleu':\n",
    "        scores = []\n",
    "        idx = 0\n",
    "\n",
    "        for pred_answer, true_answer in tqdm(zip(answers, list(dev_set['answer, str'])), desc='Evaluating with bleu'):\n",
    "            try:\n",
    "                scores.append(nltk.translate.bleu_score.sentence_bleu([true_answer], pred_answer))\n",
    "            except:\n",
    "                print(f'Error when computing bleu-score at index {idx}')\n",
    "            idx += 1\n",
    "    \n",
    "    elif metric == 'meteor':\n",
    "        scores = []\n",
    "        idx = 0\n",
    "\n",
    "        for pred_answer, true_answer in tqdm(zip(answers, list(dev_set['answer, str'])), desc='Evaluating with meteor'):\n",
    "            try:\n",
    "                scores.append(nltk.tranlsate.meteor_score([true_answer], pred_answer))\n",
    "            except:\n",
    "                print(f'Error when computing meteor-score at index {idx}')\n",
    "            idx += 1\n",
    "\n",
    "    elif metric == 'rouge':\n",
    "        scores = []\n",
    "        idx = 0\n",
    "\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "        for pred_answer, true_answer in tqdm(zip(answers, list(dev_set['answer, str'])), desc='Evaluating with rouge'):\n",
    "            try:\n",
    "                scores.append(score.score(true_answer, pred_answer)['rouge1'])\n",
    "            except:\n",
    "                print(f'Error when computing meteor-score at index {idx}')\n",
    "            idx += 1\n",
    "\n",
    "    print(f'{metric}-scores for {model_name} using {retrieval_method}: {np.mean(scores)}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
