{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/adamwagnerhoegh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, T5ForConditionalGeneration\n",
    "import torch\n",
    "from itertools import cycle\n",
    "import os \n",
    "from transformers import pipeline\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# For BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate retrieval corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(8582) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "100%|██████████| 1637/1637 [00:00<00:00, 10995.80it/s]\n"
     ]
    }
   ],
   "source": [
    "path_adam = '/Users/adamwagnerhoegh/Documents/Legal data/domsdatabasen.retsinformation_newer.json'\n",
    "path_asger = \"/Users/asgerkromand/Library/CloudStorage/OneDrive-UniversityofCopenhagen/0. SDS/1 deep learning and nlp/ANLPDP_exam/data/domsdatabasen.retsinformation_newer.json\"\n",
    "path_andreas = '' #missing\n",
    "\n",
    "# Define a function that can cycle through paths the above paths try them out, and yield the path\n",
    "def path():\n",
    "    paths = cycle([path_adam, path_asger, path_andreas])\n",
    "    for path in paths:\n",
    "        if path != '':\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                return data\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            raise FileNotFoundError('No path to data found')\n",
    "\n",
    "retsinfo = path()\n",
    "    \n",
    "rag_list = []\n",
    "idx = 0\n",
    "for lov in tqdm(retsinfo):\n",
    "    for kapitel in lov['kapitler']:\n",
    "        lov_navn = lov['shortName']\n",
    "        for paragraffer in kapitel['paragraffer']:\n",
    "            temp_paragraf_dict = {}\n",
    "            temp_paragraf_dict['paragraf_nr'] = paragraffer['nummer']\n",
    "            temp_paragraf_dict['lovnavn'] = lov_navn\n",
    "            temp_paragraf_list = []\n",
    "            for styk in paragraffer['stk']:\n",
    "                temp_paragraf_list.append(styk['tekst'])\n",
    "            temp_paragraf_dict['text'] = ' '.join(temp_paragraf_list)\n",
    "            rag_list.append(temp_paragraf_dict)\n",
    "\n",
    "with open(\"rag_list.txt\", \"w\") as file:\n",
    "    for item in rag_list:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load excel files in dev set folder\n",
    "import os\n",
    "\n",
    "dev_set_folder = \"devset\"\n",
    "\n",
    "dfs = []\n",
    "for file in os.listdir(dev_set_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(os.path.join(dev_set_folder, file))\n",
    "        dfs.append(df)\n",
    "\n",
    "# merge all excel\n",
    "dev_set = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# add csv\n",
    "rag_batch_1_with_qa = pd.read_csv(\"devset/rag_batch_1_with_qa.csv\", sep=\",\").iloc[:, 1:].dropna()\n",
    "rag_batch_1_with_qa.columns = dev_set.columns\n",
    "dev_set = pd.concat([dev_set, rag_batch_1_with_qa], ignore_index=True)\n",
    "\n",
    "# output dev set\n",
    "dev_set.to_csv(\"devset/dev_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize retrieval corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42593/42593 [00:00<00:00, 108504.70it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_list2 = rag_list\n",
    "\n",
    "def preprocess(rag_list):\n",
    "    # extract and preprocess text\n",
    "    corpus = [item['text'] for item in rag_list]\n",
    "    corpus = [re.sub('\\\\s{2,}', ' ', \n",
    "                     re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                           item.lower())) for item in corpus]\n",
    "\n",
    "    # remove stopwords\n",
    "    #nltk.download('punkt')\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    corpus = [' '.join(word for word in text.split() \n",
    "                      if word not in stop_words) for text in tqdm(corpus)]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = preprocess(rag_list2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval pipeline for BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvem skal ansøgeren eller indehaveren indgive en anmodning til for at dele en dansk varemærkeansøgning eller -registrering i to eller flere særskilte ansøgninger? \n",
      "\n",
      "[32426 32420 32394]\n",
      "§ 60 d.: For anmodning om deling af en varemærkeansøgning eller registrering betales for hver ansøgning eller registrering, der ønskes udskilt, 2.000 kr., jf. § 41, stk. 2. For en skriftlig vejledende udtalelse i henhold til § 45 b, stk. 2, betales 1.500 kr.\n",
      "§ 58.: Mister den internationale varemærkeregistrering sin gyldighed, bortfalder designeringens gyldighed her i landet på tidspunktet for bortfaldet af den internationale registrering. Har den internationale varemærkeregistrering mistet sin gyldighed på anmodning fra den oprindelige registreringsmyndighed, eller som følge af at en kontraherende part udtræder af Madridprotokollen, kan indehaveren indgive en dansk varemærkeansøgning med samme virkning, som hvis ansøgningen var indgivet på datoen for den internationale registrering eller datoen for den efterfølgende designering af Danmark, forudsat at ansøgningen indgives senest 3 måneder efter datoen for udslettelse af den internationale registrering, ansøgningen indgives senest 3 måneder efter datoen for udslettelse af den internationale registrering, ansøgningen ikke omfatter andre varer eller tjenesteydelser end dem, der var omfattet af designeringen af Danmark, ansøgningen i øvrigt opfylder kravene til en dansk varemærkeansøgning og ansøgeren betaler de foreskrevne gebyrer, jf. § 60 a, stk. 4. En ansøgning om videreførelse i Danmark efter stk. 2 skal opfylde kravene til ansøgninger, jf. § 11, stk. 5-8, og indeholde en henvisning til nummeret på den internationale registrering og oplysning om designeringsdatoen og eventuel prioritet for den internationale registrering.\n",
      "§ 41.: Ansøgeren eller indehaveren kan dele en dansk varemærkeansøgning eller -registrering i to eller flere særskilte ansøgninger eller registreringer ved at indgive en anmodning herom til Patent- og Varemærkestyrelsen og for hver udskilt ansøgning eller registrering anføre, hvilke varer eller tjenesteydelser fra den oprindelige ansøgning eller registrering der skal omfattes af de udskilte ansøgninger eller registreringer. For anmodning om deling efter stk. 1 betales et gebyr, jf. § 60 d, stk. 1.\n"
     ]
    }
   ],
   "source": [
    "def sparse_retrieval(question, sparse_matrix, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # preprocess and vectorize question\n",
    "    question_processed = [re.sub('\\\\s{2,}', ' ', \n",
    "                               re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                                     question.lower()))]\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_processed = [' '.join(word for word in text.split() \n",
    "                                 if word not in stop_words) for text in question_processed]\n",
    "    \n",
    "    question_vector = vectorizer.transform(question_processed)\n",
    "\n",
    "    # sparse retrieval (cosine similarity)\n",
    "    sparse_retrieval = sparse_matrix.dot(question_vector.T).toarray()\n",
    "\n",
    "    # get top k paragraphs\n",
    "    top_k = np.argsort(sparse_retrieval.flatten())[-k:]\n",
    "\n",
    "    return top_k\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k = sparse_retrieval(random_question, X)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list2[i][\"paragraf_nr\"]}: {rag_list2[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hvem skal skal i tilfælde af væsentlig forurening eller overhængende fare for væsentlig forurening straks underrette tilsynsmyndigheden om alle relevante aspekter af situationen?\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bm25' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Assuming bm25 is the initialized BM25Okapi model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m top_k_results \u001b[38;5;241m=\u001b[39m bm25_retrieval(random_question, \u001b[43mbm25\u001b[49m, rag_list2, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Print top-k results\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paragraf_nr, text, score \u001b[38;5;129;01min\u001b[39;00m top_k_results:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bm25' is not defined"
     ]
    }
   ],
   "source": [
    "def bm25_retrieval(question, bm25_model, rag_list, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of the most relevant paragraphs based on BM25.\n",
    "    \"\"\"\n",
    "    # Preprocess and tokenize the question\n",
    "    question_processed = re.sub(r'\\s{2,}', ' ', \n",
    "                                 re.sub(r'\\W|[0-9]|§', ' ', question.lower()))\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_tokens = [word for word in question_processed.split() if word not in stop_words]\n",
    "\n",
    "    # Get BM25 scores for the query\n",
    "    scores = bm25_model.get_scores(question_tokens)\n",
    "\n",
    "    # Get the top k results\n",
    "    top_k_indices = np.argsort(scores)[-k:][::-1]  # Sort scores in descending order\n",
    "\n",
    "    # Return the top k paragraphs\n",
    "    return [(rag_list[i]['paragraf_nr'], rag_list[i]['text'], scores[i]) for i in top_k_indices]\n",
    "\n",
    "# Example Usage\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(f\"Question: {random_question}\\n\")\n",
    "\n",
    "# Assuming bm25 is the initialized BM25Okapi model\n",
    "top_k_results = bm25_retrieval(random_question, bm25, rag_list2, k=3)\n",
    "\n",
    "# Print top-k results\n",
    "for paragraf_nr, text, score in top_k_results:\n",
    "    print(f\"{paragraf_nr}: {text} (Score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embedding_matrix(pooling, save=True, save_folder=None):\n",
    "    # initialise model\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "    bert_model = AutoModel.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "\n",
    "    # define device\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # move model to device\n",
    "    bert_model.to(device)\n",
    "\n",
    "    # create list of embedding vectors to concatenate into a torch tensor\n",
    "    embeddings = []\n",
    "\n",
    "    # index to track numer of mistakes\n",
    "    idx = 0\n",
    "\n",
    "    for item in tqdm(rag_list):\n",
    "        # doing a try and except as some paragraphs may exceed the context window of the BERT (I believe)\n",
    "        try:\n",
    "            # tokenize texts\n",
    "            input_ids = bert_tokenizer.encode(item['text'], return_tensors='pt').to(device)\n",
    "            # run through BERT\n",
    "            with torch.no_grad():  # disable gradient computation for inference\n",
    "                outputs = bert_model(input_ids)\n",
    "            \n",
    "            # different kinds of pooling\n",
    "            if pooling == 'cls':\n",
    "                embedding_vector = outputs.last_hidden_state[:, 0, :]\n",
    "            elif pooling == 'max':\n",
    "                embedding_vector = torch.max(outputs, dim=1)[0]\n",
    "            elif pooling == 'mean':\n",
    "                embedding_vector = torch.mean(outputs, dim=1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling method: {pooling}\")\n",
    "            \n",
    "            # add cls-vector to list of embeddings\n",
    "            embeddings.append(embedding_vector)\n",
    "        except:\n",
    "            # if error then count errors with this\n",
    "            idx += 1\n",
    "\n",
    "    print(f'{idx} no. of errors')\n",
    "\n",
    "    # concatenate list into torch tensor\n",
    "    embeddings_tensor = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    if save == True:\n",
    "        # make sure that folder exists\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "        # save tensor \n",
    "        torch.save(embeddings_tensor, f'{save_folder}/{pooling}_embeddings_tensor.pt')\n",
    "\n",
    "    return embeddings_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_embedding_matrix(pooling='cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at vesteinn/DanskBERT and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "bert_model = AutoModel.from_pretrained(\"vesteinn/DanskBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = torch.load('/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_DanskBERT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvem kan tillade, at der meddeles personer, som ikke opfylder de almindelige førlighedsbestemmelser, særlige kørekort? \n",
      "\n",
      "Statens tilskud til den enkelte kommunes små storbyteatre fastsættes under et i en 4-årig aftale mellem staten og kommunen. Tilskud til husleje kan kun udgøre indtil 15 pct. af det tilskudsberettigede kommunale driftstilskud til det enkelte lille storbyteater. Statstilskuddet ydes kun, såfremt kommunen har indgået minimum 3-årige aftaler om driftstilskud med de enkelte teatre. Kulturministeren fastsætter efter forhandling med Kommunernes Landsforening nærmere regler om ordningen for små storbyteatre. Kulturministeren kan i forbindelse med kulturaftaler med kommuner m.v. fravige bestemmelserne i stk. 1-4 og i § 16 a.\n",
      "Inflationshjælp efter § 1, som er udbetalt til en person, der ikke er berettiget til hjælpen, eller dennes dødbo, skal ikke tilbagebetales, hvis udbetalingen skyldes en myndighedsfejl og det samlede udbetalte beløb ikke overstiger 13.500 kr., jf. dog stk. 2. Inflationshjælp efter § 1 skal tilbagebetales, når en person skal tilbagebetale hele forsørgelsesydelsen nævnt i § 1, stk. 1, nr. 1, for januar 2023, jf. § 91, stk. 1, nr. 2, i lov om aktiv socialpolitik. Tilbagebetalingen opkræves af kommunen. Et tilbagebetalingskrav bortfalder 3 år efter udbetalingen af inflationshjælp. Beskæftigelsesministeren kan efter forhandling med skatteministeren fastsætte regler om en afdragsordning for tilbagebetaling, jf. stk. 2, herunder at en tilbagebetalingsaftale bortfalder, hvis skyldneren trods påkrav udebliver med tilbagebetaling af inflati‍onshjælpen.\n",
      "Hvis en virksomheds tilsvar af skatter og afgifter m.v., der opkræves efter reglerne i denne lov, for en afregningsperiode er negativt, udbetales beløbet til virksomheden. Såfremt angivelsen henholdsvis indberetningen af beløb omfattet af § 2, stk. 1, 4. pkt. er modtaget rettidigt, sker udbetaling efter stk. 1 senest 3 uger efter modtagelsen af angivelsen henholdsvis indberetningen for den pågældende periode. Kan told- og skatteforvaltningen på grund af virksomhedens forhold ikke foretage kontrol af angivelsen eller indberetningen af beløb omfattet af § 2, stk. 1, 4. pkt., afbrydes udbetalingsfristen, indtil virksomhedens forhold ikke længere hindrer kontrol. Beløb, der skulle have været udbetalt efter stk. 1, kan tilbageholdes, såfremt angivelser eller indberetningen af beløb omfattet af § 2, stk. 1, 4. pkt. vedrørende afsluttede afregningsperioder ikke er indgivet. Negative tilsvar efter stk. 1, der indgår ved en samlet kontoopgørelse af virksomhedens tilsvar af skatter og afgifter m.v. efter reglerne i denne lovs kapitel 5, kan alene udbetales, hvis det negative tilsvar modsvares af en kreditsaldo opgjort efter § 16 a, stk. 2, 2. pkt.\n"
     ]
    }
   ],
   "source": [
    "def dense_retrieval(question, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the input sentence\n",
    "    input_ids = bert_tokenizer.encode(question, return_tensors=\"pt\")  # Encode and add batch dimension\n",
    "    # Pass the input through the model\n",
    "    \n",
    "    with torch.no_grad():  # disable gradient computation for inference\n",
    "        outputs = bert_model(input_ids)\n",
    "\n",
    "    # Extract the CLS token representation\n",
    "    cls_vector = outputs.last_hidden_state[:, 0, :]  # CLS token is at position 0\n",
    "    \n",
    "    # sparse retrieval (cosine similarity)\n",
    "    dense_retrieval = embeddings_matrix @ torch.transpose(cls_vector, 0, 1)\n",
    "    \n",
    "    # get top k paragraphs\n",
    "    top_k_indices = torch.sort(dense_retrieval, descending=True, dim=0)[1][:k]\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k = dense_retrieval(random_question, k=3)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name, retrieval_method, metric, k):\n",
    "    \"\"\"\n",
    "    model_name = 'KennethTM/gpt-neo-1.3B-danish' or 'strombergnlp/dant5-large'\n",
    "    retrieval_method = 'tf-idf', 'bm25' or 'dense'\n",
    "    metric = 'bleu', 'rouge' or 'meteor'\n",
    "    \"\"\"\n",
    "    # set device to mps\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # load AutoTokenizer for model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # generate list of answers to fill\n",
    "    answers = []\n",
    "\n",
    "    # load neo\n",
    "    # the loops are made for each model to not waste compute on loading models for each question\n",
    "\n",
    "    if model_name == 'KennethTM/gpt-neo-1.3B-danish':\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"KennethTM/gpt-neo-1.3B-danish\").to(device)\n",
    "    \n",
    "        for item in dev_set:\n",
    "            # get question\n",
    "            question = item['question']\n",
    "\n",
    "            if retrieval_method == 'tf-idf':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in sparse_retrieval(dev_set['question, str'][item], X, k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            elif retrieval_method == 'bm25':\n",
    "                paragraphs = bm25_retrieval(rag_list=rag_list, k=3)\n",
    "            \n",
    "            elif retrieval_method == 'dense_retrieval':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in dense_retrieval(dev_set['question, str'][item], k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            # assemble all in prompt\n",
    "            prompt = f'Kontekst: {paragraphs} Spørgsmål: {question} Svar: '\n",
    "\n",
    "            # tokenize\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # set max_length to no. of tokens in prompt + 100 (the 100 are thus for generation)\n",
    "            max_length = int(input_ids['input_ids'].size(1)) + 100\n",
    "\n",
    "            # generate answer with no_grad() to save compute\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=max_length,\n",
    "                    pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            # decode the generated answer\n",
    "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            answers.append(answer)\n",
    "    \n",
    "\n",
    "    # load T5\n",
    "    elif model_name == 'strombergnlp/dant5-large':\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"strombergnlp/dant5-large\").to(device)\n",
    "\n",
    "        for item in dev_set:\n",
    "            # get question\n",
    "            question = item['question']\n",
    "\n",
    "            if retrieval_method == 'tf-idf':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in sparse_retrieval(dev_set['question, str'][item], X, k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            elif retrieval_method == 'bm25':\n",
    "                paragraphs = bm25_retrieval(rag_list=rag_list, k=3)\n",
    "            \n",
    "            elif retrieval_method == 'dense_retrieval':\n",
    "                # create list of paragraphs by getting indexes for best hits with sparse_retrieval\n",
    "                paragraphs = [rag_list[i]['text'] for i in dense_retrieval(dev_set['question, str'][item], k=3)]\n",
    "                # join list into long string\n",
    "                paragraphs = ' '.join(paragraphs)\n",
    "\n",
    "            # assemble all in input\n",
    "            input_text = f\"Spørgsmål: {question} Kontekst: {paragraphs} Svar:\"\n",
    "\n",
    "            # tokenize\n",
    "            input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "            # generate answer with no_grad() to save compute\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=max_length,\n",
    "                    pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(input_ids, max_length=100)\n",
    "\n",
    "            # Decode and print the generated answer\n",
    "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            answers.append(answer)\n",
    "        \n",
    "    \n",
    "    if metric == 'bleu':\n",
    "        scores = []\n",
    "        idx = 0\n",
    "\n",
    "        for pred_answer, true_answer in zip(answers, list(dev_set['answer, str'])):\n",
    "            try:\n",
    "                bleu_scores.append(nltk.translate.bleu_score.sentence_bleu([true_answer], pred_answer))\n",
    "            except:\n",
    "                print(f'Error when computing bleu-score at index {idx}')\n",
    "            idx += 1\n",
    "    \n",
    "    elif metric == 'meteor':\n",
    "        scores = []\n",
    "        idx = 0\n",
    "\n",
    "        for pred_answer, true_answer in zip(answers, list(dev_set['answer, str'])):\n",
    "            try:\n",
    "                meteor_scores.append(nltk.tranlsate.meteor_score([true_answer], pred_answer))\n",
    "            except:\n",
    "                print(f'Error when computing meteor-score at index {idx}')\n",
    "            idx += 1\n",
    "\n",
    "    elif metric == 'rouge':\n",
    "        scores = []\n",
    "        idx = 0\n",
    "\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "        for pred_answer, true_answer in zip(answers, list(dev_set['answer, str'])):\n",
    "            try:\n",
    "                rouge_scores.append(score.score(true_answer, pred_answer)['rouge1'])\n",
    "            except:\n",
    "                print(f'Error when computing meteor-score at index {idx}')\n",
    "            idx += 1\n",
    "\n",
    "    print(f'{metric}-scores for {model_name} using {retrieval_method}: {np.mean(scores)}')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # insert code that measures amount of tokens in prepended paragraphs and adds 100 for generation.\n",
    "    \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
