{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate retrieval corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1637/1637 [00:00<00:00, 11257.12it/s]\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/adamwagnerhoegh/Documents/Legal data/domsdatabasen.retsinformation_newer.json'\n",
    "\n",
    "with open(path) as f:\n",
    "    retsinfo = json.load(f)\n",
    "\n",
    "rag_list = []\n",
    "idx = 0\n",
    "for lov in tqdm(retsinfo):\n",
    "    for kapitel in lov['kapitler']:\n",
    "        lov_navn = lov['shortName']\n",
    "        for paragraffer in kapitel['paragraffer']:\n",
    "            temp_paragraf_dict = {}\n",
    "            temp_paragraf_dict['paragraf_nr'] = paragraffer['nummer']\n",
    "            temp_paragraf_dict['lovnavn'] = lov_navn\n",
    "            temp_paragraf_list = []\n",
    "            for styk in paragraffer['stk']:\n",
    "                temp_paragraf_list.append(styk['tekst'])\n",
    "            temp_paragraf_dict['text'] = ' '.join(temp_paragraf_list)\n",
    "            rag_list.append(temp_paragraf_dict)\n",
    "\n",
    "with open(\"rag_list.txt\", \"w\") as file:\n",
    "    for item in rag_list:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question, str</th>\n",
       "      <th>answer, str</th>\n",
       "      <th>text, str</th>\n",
       "      <th>pnumber, str</th>\n",
       "      <th>law number, str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hvad har ejeren af en ejerlejlighed, sammen me...</td>\n",
       "      <td>Grunden, fælles bestanddele og tilbehør</td>\n",
       "      <td>'Ejeren af en ejerlejlighed har sammen med and...</td>\n",
       "      <td>3</td>\n",
       "      <td>LOV nr 908 af 18/06/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hvem fastsætter eller aftaler bestemmelser om ...</td>\n",
       "      <td>Finansministeren fastsætter eller aftaler best...</td>\n",
       "      <td>'Højskolen skal følge de af finansministeren f...</td>\n",
       "      <td>30</td>\n",
       "      <td>LBK nr 780 af 08/08/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hvad skal Beskæftigelsesministeriet og Finanst...</td>\n",
       "      <td>Den indsendte årsrapport skal i det mindste in...</td>\n",
       "      <td>'Uden ugrundet ophold efter repræsentantskabet...</td>\n",
       "      <td>25 l</td>\n",
       "      <td>LBK nr 1110 af 10/10/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hvor mange procent må kapitalandele i og lån y...</td>\n",
       "      <td>Kapitalandele i og lån ydet til en virksomhed ...</td>\n",
       "      <td>'Følgende grænser for Arbejdsmarkedets Tillægs...</td>\n",
       "      <td>26 e</td>\n",
       "      <td>LBK nr 1110 af 10/10/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hvad er en betingelse for retten til jobpræmie?</td>\n",
       "      <td>Det er en betingelse for retten til jobpræmie ...</td>\n",
       "      <td>'Det er en betingelse for retten til jobpræmie...</td>\n",
       "      <td>9</td>\n",
       "      <td>LOV nr 287 af 29/03/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Hvordan anføres kandidatlister på stemmesedler?</td>\n",
       "      <td>I særskilte felter.</td>\n",
       "      <td>Kandidatlisterne anføres på stemmesedlen i sær...</td>\n",
       "      <td>46</td>\n",
       "      <td>LBK nr 6 af 08/01/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Hvem iværksætter beslaglæggelse?</td>\n",
       "      <td>Politiet.</td>\n",
       "      <td>Politiet iværksætter beslaglæggelse. Politiet ...</td>\n",
       "      <td>807</td>\n",
       "      <td>LBK nr 250 af 04/03/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Hvis interesser skal foranstaltninger mod inte...</td>\n",
       "      <td>De forvaltede alternative investeringsfondes e...</td>\n",
       "      <td>En forvalter af alternative investeringsfonde ...</td>\n",
       "      <td>23</td>\n",
       "      <td>LBK nr 231 af 01/03/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Hvad skal valgstyrere eller tilforordnede vælg...</td>\n",
       "      <td>At stemmekasserne er tomme.</td>\n",
       "      <td>Afstemningen begynder kl. Inden stemmeafgivnin...</td>\n",
       "      <td>38</td>\n",
       "      <td>LBK nr 1432 af 01/12/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Hvem fastsætter om udenlandske afgørelser af b...</td>\n",
       "      <td>Justitsministeren.</td>\n",
       "      <td>Justitsministeren kan fastsætte bestemmelser, ...</td>\n",
       "      <td>223 a</td>\n",
       "      <td>LBK nr 250 af 04/03/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         question, str  \\\n",
       "0    Hvad har ejeren af en ejerlejlighed, sammen me...   \n",
       "1    Hvem fastsætter eller aftaler bestemmelser om ...   \n",
       "2    Hvad skal Beskæftigelsesministeriet og Finanst...   \n",
       "3    Hvor mange procent må kapitalandele i og lån y...   \n",
       "4      Hvad er en betingelse for retten til jobpræmie?   \n",
       "..                                                 ...   \n",
       "101    Hvordan anføres kandidatlister på stemmesedler?   \n",
       "102                   Hvem iværksætter beslaglæggelse?   \n",
       "103  Hvis interesser skal foranstaltninger mod inte...   \n",
       "104  Hvad skal valgstyrere eller tilforordnede vælg...   \n",
       "105  Hvem fastsætter om udenlandske afgørelser af b...   \n",
       "\n",
       "                                           answer, str  \\\n",
       "0              Grunden, fælles bestanddele og tilbehør   \n",
       "1    Finansministeren fastsætter eller aftaler best...   \n",
       "2    Den indsendte årsrapport skal i det mindste in...   \n",
       "3    Kapitalandele i og lån ydet til en virksomhed ...   \n",
       "4    Det er en betingelse for retten til jobpræmie ...   \n",
       "..                                                 ...   \n",
       "101                                I særskilte felter.   \n",
       "102                                          Politiet.   \n",
       "103  De forvaltede alternative investeringsfondes e...   \n",
       "104                        At stemmekasserne er tomme.   \n",
       "105                                 Justitsministeren.   \n",
       "\n",
       "                                             text, str pnumber, str  \\\n",
       "0    'Ejeren af en ejerlejlighed har sammen med and...            3   \n",
       "1    'Højskolen skal følge de af finansministeren f...           30   \n",
       "2    'Uden ugrundet ophold efter repræsentantskabet...         25 l   \n",
       "3    'Følgende grænser for Arbejdsmarkedets Tillægs...         26 e   \n",
       "4    'Det er en betingelse for retten til jobpræmie...            9   \n",
       "..                                                 ...          ...   \n",
       "101  Kandidatlisterne anføres på stemmesedlen i sær...           46   \n",
       "102  Politiet iværksætter beslaglæggelse. Politiet ...          807   \n",
       "103  En forvalter af alternative investeringsfonde ...           23   \n",
       "104  Afstemningen begynder kl. Inden stemmeafgivnin...           38   \n",
       "105  Justitsministeren kan fastsætte bestemmelser, ...        223 a   \n",
       "\n",
       "               law number, str  \n",
       "0     LOV nr 908 af 18/06/2020  \n",
       "1     LBK nr 780 af 08/08/2019  \n",
       "2    LBK nr 1110 af 10/10/2014  \n",
       "3    LBK nr 1110 af 10/10/2014  \n",
       "4     LOV nr 287 af 29/03/2017  \n",
       "..                         ...  \n",
       "101     LBK nr 6 af 08/01/2024  \n",
       "102   LBK nr 250 af 04/03/2024  \n",
       "103   LBK nr 231 af 01/03/2024  \n",
       "104  LBK nr 1432 af 01/12/2023  \n",
       "105   LBK nr 250 af 04/03/2024  \n",
       "\n",
       "[106 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load excel files in dev set folder\n",
    "import os\n",
    "\n",
    "dev_set_folder = \"devset\"\n",
    "\n",
    "dfs = []\n",
    "for file in os.listdir(dev_set_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(os.path.join(dev_set_folder, file))\n",
    "        dfs.append(df)\n",
    "\n",
    "# merge all excel\n",
    "dev_set = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# add csv\n",
    "rag_batch_1_with_qa = pd.read_csv(\"devset/rag_batch_1_with_qa.csv\", sep=\";\").iloc[:, 1:].dropna()\n",
    "rag_batch_1_with_qa.columns = dev_set.columns\n",
    "dev_set = pd.concat([dev_set, rag_batch_1_with_qa], ignore_index=True)\n",
    "\n",
    "dev_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize retrieval corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42593/42593 [00:00<00:00, 103915.01it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_list2 = rag_list\n",
    "\n",
    "def preprocess(rag_list):\n",
    "    # extract and preprocess text\n",
    "    corpus = [item['text'] for item in rag_list]\n",
    "    corpus = [re.sub('\\\\s{2,}', ' ', \n",
    "                     re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                           item.lower())) for item in corpus]\n",
    "\n",
    "    # remove stopwords\n",
    "    #nltk.download('punkt')\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    corpus = [' '.join(word for word in text.split() \n",
    "                      if word not in stop_words) for text in tqdm(corpus)]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = preprocess(rag_list2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvad skal sikringsmæssige foranstaltninger for nukleart materiale og nukleare anlæg, jf. § 37 a og § 37 c, stk. 2, skal fremgå af? \n",
      "\n",
      "§ 37 b.: Sikringsmæssige foranstaltninger for nukleart materiale og nukleare anlæg, jf. § 37 a og § 37 c, stk. 2, skal fremgå af en sikringsplan, som skal indsendes til og godkendes af Beredskabsstyrelsen forud for brug, opbevaring eller transport af nukleart materiale og forud for drift af et nukleart anlæg. Ændrer de forhold, der har ligget til grund for godkendelse af en sikringsplan, sig væsentligt efterfølgende, skal en revideret sikringsplan godkendes af Beredskabsstyrelsen. Beredskabsstyrelsen kan træffe afgørelse om, at godkendelse af en sikringsplan bortfalder, såfremt den, der bruger, opbevarer eller transporterer nukleart materiale, eller den, der driver et nukleart anlæg, ikke inden en nærmere angivet frist har indsendt og opnået godkendelse af en revideret sikringsplan.\n",
      "§ 37 a.: Den, der bruger, opbevarer eller transporterer nukleart materiale, som udnyttes til fredelige formål, skal træffe sikringsmæssige foranstaltninger mod tyveri eller anden uretmæssig tilegnelse af materialet. Den, der bruger, opbevarer eller transporterer nukleart materiale, som udnyttes til fredelige formål, og den, der driver et nukleart anlæg, skal træffe sikringsmæssige foranstaltninger til beskyttelse af materialet og anlægget mod sabotage.\n",
      "§ 37 c.: Beredskabsstyrelsen fører tilsyn med overholdelse af forpligtelserne efter §§ 37 a og 37 b og regler, som udstedes i medfør af stk. 2. Beredskabsstyrelsen fastsætter nærmere regler om sikringsmæssige foranstaltninger for nukleart materiale og nukleare anlæg, jf. § 37 a, og om sikringsplaner, jf. § 37 b.\n"
     ]
    }
   ],
   "source": [
    "def sparse_retrieval(question, sparse_matrix, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # preprocess and vectorize question\n",
    "    question_processed = [re.sub('\\\\s{2,}', ' ', \n",
    "                               re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                                     question.lower()))]\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_processed = [' '.join(word for word in text.split() \n",
    "                                 if word not in stop_words) for text in question_processed]\n",
    "    \n",
    "    question_vector = vectorizer.transform(question_processed)\n",
    "\n",
    "    # sparse retrieval (cosine similarity)\n",
    "    sparse_retrieval = X.dot(question_vector.T).toarray()\n",
    "\n",
    "    # get top k paragraphs\n",
    "    top_k = np.argsort(sparse_retrieval.flatten())[-k:]\n",
    "\n",
    "    return top_k\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k = sparse_retrieval(random_question, X)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list2[i][\"paragraf_nr\"]}: {rag_list2[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at KennethTM/bert-base-uncased-danish and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"KennethTM/bert-base-uncased-danish\")\n",
    "model = AutoModel.from_pretrained(\"KennethTM/bert-base-uncased-danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 87/42593 [00:04<29:04, 24.37it/s] Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 42593/42593 [30:10<00:00, 23.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955 no. of errors\n"
     ]
    }
   ],
   "source": [
    "# creating embedding corpus\n",
    "\n",
    "\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# cls_embeddings = []\n",
    "\n",
    "# idx = 0\n",
    "\n",
    "# for item in tqdm(rag_list):\n",
    "#     # doing a try and except as some paragraphs may exceed the context window of the BERT (I believe)\n",
    "#     try:\n",
    "#         # tokenize texts\n",
    "#         input_ids = tokenizer.encode(item['text'], return_tensors='pt')\n",
    "#         # run through BERT\n",
    "#         with torch.no_grad():  # disable gradient computation for inference\n",
    "#             outputs = model(input_ids)\n",
    "#         # extract cls-token\n",
    "#         cls_vector = outputs.last_hidden_state[:, 0, :]\n",
    "#         # add cls-vector to list of embeddings\n",
    "#         cls_embeddings.append(cls_vector)\n",
    "#     except:\n",
    "#         # if error then count errors with this\n",
    "#         idx += 1\n",
    "\n",
    "# print(f'{idx} no. of errors')\n",
    "\n",
    "# # concatenate list into torch tensor\n",
    "# cls_embeddings_tensor = torch.cat(cls_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the tensor\n",
    "#torch.save(cls_embeddings_tensor, '/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(rag_list[0]['text'], return_tensors='pt')\n",
    "outputs = model(input_ids)\n",
    "cls_vector = outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense retrieval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at KennethTM/bert-base-uncased-danish and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sydslesvigudvalget sender for hvert finansår inden den 1. maj det følgende år et årsregnskab, der underskrives af hvem? \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m random_question \u001b[38;5;241m=\u001b[39m dev_set\u001b[38;5;241m.\u001b[39miloc[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dev_set))][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion, str\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(random_question, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m top_k \u001b[38;5;241m=\u001b[39m dense_retrieval(random_question, \u001b[43mX\u001b[49m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_k:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrag_list2[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparagraf_nr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrag_list2[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def dense_retrieval(question, dense_matrix, k=3):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode the input sentence\n",
    "    input_ids = tokenizer.encode(question, return_tensors=\"pt\")  # Encode and add batch dimension\n",
    "    # Pass the input through the model\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "    # Extract the CLS token representation\n",
    "    cls_vector = outputs.last_hidden_state[:, 0, :]  # CLS token is at position 0\n",
    "    \n",
    "    # sparse retrieval (cosine similarity)\n",
    "    dense_retrieval = embedding_corpus.dot(cls_vector.T).toarray()\n",
    "\n",
    "    # get top k paragraphs\n",
    "    top_k = np.argsort(sparse_retrieval.flatten())[-k:]\n",
    "\n",
    "    return top_k\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question, str']\n",
    "print(random_question, '\\n')\n",
    "top_k = dense_retrieval(random_question, X)\n",
    "for i in top_k:\n",
    "    print(f'{rag_list2[i][\"paragraf_nr\"]}: {rag_list2[i][\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m cls_vector \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# CLS token is at position 0\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# sparse retrieval (cosine similarity)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m sparse_retrieval \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241m.\u001b[39mdot(cls_vector\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# get top k paragraphs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m top_k \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(sparse_retrieval\u001b[38;5;241m.\u001b[39mflatten())[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Encode the input sentence\n",
    "input_ids = tokenizer.encode('Hvem fastsætter eller aftaler bestemmelser om løn- og ansættelsesvilkår, herunder pensionsforhold for højskolens ansatte?', return_tensors=\"pt\")  # Encode and add batch dimension\n",
    "# Pass the input through the model\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Extract the CLS token representation\n",
    "cls_vector = outputs.last_hidden_state[:, 0, :]  # CLS token is at position 0\n",
    "\n",
    "# sparse retrieval (cosine similarity)\n",
    "sparse_retrieval = X.dot(cls_vector.T).toarray()\n",
    "\n",
    "# get top k paragraphs\n",
    "top_k = np.argsort(sparse_retrieval.flatten())[-3:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
