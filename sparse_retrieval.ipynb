{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import random\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate retrieval corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1637/1637 [00:00<00:00, 8591.64it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/domsdatabasen.retsinformation_newer.json') as f:\n",
    "    retsinfo = json.load(f)\n",
    "\n",
    "rag_list = []\n",
    "idx = 0\n",
    "for lov in tqdm(retsinfo):\n",
    "    for kapitel in lov['kapitler']:\n",
    "        lov_navn = lov['shortName']\n",
    "        for paragraffer in kapitel['paragraffer']:\n",
    "            temp_paragraf_dict = {}\n",
    "            temp_paragraf_dict['paragraf_nr'] = paragraffer['nummer']\n",
    "            temp_paragraf_dict['lovnavn'] = lov_navn\n",
    "            temp_paragraf_list = []\n",
    "            for styk in paragraffer['stk']:\n",
    "                temp_paragraf_list.append(styk['tekst'])\n",
    "            temp_paragraf_dict['text'] = ' '.join(temp_paragraf_list)\n",
    "            rag_list.append(temp_paragraf_dict)\n",
    "\n",
    "with open(\"rag_list.txt\", \"w\") as file:\n",
    "    for item in rag_list:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and merge excel files from devset folder\n",
    "dfs = [pd.read_excel(os.path.join(\"devset\", f)) for f in os.listdir(\"devset\") if f.endswith(\".xlsx\")]\n",
    "dev_set = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# add csv data\n",
    "csv_data = pd.read_csv(\"devset/rag_batch_1_with_qa.csv\", sep=\";\").iloc[:, 1:].dropna()\n",
    "csv_data.columns = dev_set.columns\n",
    "dev_set = pd.concat([dev_set, csv_data], ignore_index=True)\n",
    "\n",
    "# change column names\n",
    "dev_set.columns = ['question', 'answer', 'text', 'paragraph', 'law']\n",
    "\n",
    "# write to csv\n",
    "dev_set.to_csv(\"data/dev_set.csv\", index=False)\n",
    "\n",
    "### THIS CAN BE DELETED NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>text</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>law</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hvad har ejeren af en ejerlejlighed, sammen me...</td>\n",
       "      <td>Grunden, fælles bestanddele og tilbehør</td>\n",
       "      <td>'Ejeren af en ejerlejlighed har sammen med and...</td>\n",
       "      <td>3</td>\n",
       "      <td>LOV nr 908 af 18/06/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hvem fastsætter eller aftaler bestemmelser om ...</td>\n",
       "      <td>Finansministeren fastsætter eller aftaler best...</td>\n",
       "      <td>'Højskolen skal følge de af finansministeren f...</td>\n",
       "      <td>30</td>\n",
       "      <td>LBK nr 780 af 08/08/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hvad skal Beskæftigelsesministeriet og Finanst...</td>\n",
       "      <td>Den indsendte årsrapport skal i det mindste in...</td>\n",
       "      <td>'Uden ugrundet ophold efter repræsentantskabet...</td>\n",
       "      <td>25 l</td>\n",
       "      <td>LBK nr 1110 af 10/10/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hvor mange procent må kapitalandele i og lån y...</td>\n",
       "      <td>Kapitalandele i og lån ydet til en virksomhed ...</td>\n",
       "      <td>'Følgende grænser for Arbejdsmarkedets Tillægs...</td>\n",
       "      <td>26 e</td>\n",
       "      <td>LBK nr 1110 af 10/10/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hvad er en betingelse for retten til jobpræmie?</td>\n",
       "      <td>Det er en betingelse for retten til jobpræmie ...</td>\n",
       "      <td>'Det er en betingelse for retten til jobpræmie...</td>\n",
       "      <td>9</td>\n",
       "      <td>LOV nr 287 af 29/03/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Hvordan anføres kandidatlister på stemmesedler?</td>\n",
       "      <td>I særskilte felter.</td>\n",
       "      <td>Kandidatlisterne anføres på stemmesedlen i sær...</td>\n",
       "      <td>46</td>\n",
       "      <td>LBK nr 6 af 08/01/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Hvem iværksætter beslaglæggelse?</td>\n",
       "      <td>Politiet.</td>\n",
       "      <td>Politiet iværksætter beslaglæggelse. Politiet ...</td>\n",
       "      <td>807</td>\n",
       "      <td>LBK nr 250 af 04/03/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Hvis interesser skal foranstaltninger mod inte...</td>\n",
       "      <td>De forvaltede alternative investeringsfondes e...</td>\n",
       "      <td>En forvalter af alternative investeringsfonde ...</td>\n",
       "      <td>23</td>\n",
       "      <td>LBK nr 231 af 01/03/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Hvad skal valgstyrere eller tilforordnede vælg...</td>\n",
       "      <td>At stemmekasserne er tomme.</td>\n",
       "      <td>Afstemningen begynder kl. Inden stemmeafgivnin...</td>\n",
       "      <td>38</td>\n",
       "      <td>LBK nr 1432 af 01/12/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Hvem fastsætter om udenlandske afgørelser af b...</td>\n",
       "      <td>Justitsministeren.</td>\n",
       "      <td>Justitsministeren kan fastsætte bestemmelser, ...</td>\n",
       "      <td>223 a</td>\n",
       "      <td>LBK nr 250 af 04/03/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    Hvad har ejeren af en ejerlejlighed, sammen me...   \n",
       "1    Hvem fastsætter eller aftaler bestemmelser om ...   \n",
       "2    Hvad skal Beskæftigelsesministeriet og Finanst...   \n",
       "3    Hvor mange procent må kapitalandele i og lån y...   \n",
       "4      Hvad er en betingelse for retten til jobpræmie?   \n",
       "..                                                 ...   \n",
       "101    Hvordan anføres kandidatlister på stemmesedler?   \n",
       "102                   Hvem iværksætter beslaglæggelse?   \n",
       "103  Hvis interesser skal foranstaltninger mod inte...   \n",
       "104  Hvad skal valgstyrere eller tilforordnede vælg...   \n",
       "105  Hvem fastsætter om udenlandske afgørelser af b...   \n",
       "\n",
       "                                                answer  \\\n",
       "0              Grunden, fælles bestanddele og tilbehør   \n",
       "1    Finansministeren fastsætter eller aftaler best...   \n",
       "2    Den indsendte årsrapport skal i det mindste in...   \n",
       "3    Kapitalandele i og lån ydet til en virksomhed ...   \n",
       "4    Det er en betingelse for retten til jobpræmie ...   \n",
       "..                                                 ...   \n",
       "101                                I særskilte felter.   \n",
       "102                                          Politiet.   \n",
       "103  De forvaltede alternative investeringsfondes e...   \n",
       "104                        At stemmekasserne er tomme.   \n",
       "105                                 Justitsministeren.   \n",
       "\n",
       "                                                  text paragraph  \\\n",
       "0    'Ejeren af en ejerlejlighed har sammen med and...         3   \n",
       "1    'Højskolen skal følge de af finansministeren f...        30   \n",
       "2    'Uden ugrundet ophold efter repræsentantskabet...      25 l   \n",
       "3    'Følgende grænser for Arbejdsmarkedets Tillægs...      26 e   \n",
       "4    'Det er en betingelse for retten til jobpræmie...         9   \n",
       "..                                                 ...       ...   \n",
       "101  Kandidatlisterne anføres på stemmesedlen i sær...        46   \n",
       "102  Politiet iværksætter beslaglæggelse. Politiet ...       807   \n",
       "103  En forvalter af alternative investeringsfonde ...        23   \n",
       "104  Afstemningen begynder kl. Inden stemmeafgivnin...        38   \n",
       "105  Justitsministeren kan fastsætte bestemmelser, ...     223 a   \n",
       "\n",
       "                           law  \n",
       "0     LOV nr 908 af 18/06/2020  \n",
       "1     LBK nr 780 af 08/08/2019  \n",
       "2    LBK nr 1110 af 10/10/2014  \n",
       "3    LBK nr 1110 af 10/10/2014  \n",
       "4     LOV nr 287 af 29/03/2017  \n",
       "..                         ...  \n",
       "101     LBK nr 6 af 08/01/2024  \n",
       "102   LBK nr 250 af 04/03/2024  \n",
       "103   LBK nr 231 af 01/03/2024  \n",
       "104  LBK nr 1432 af 01/12/2023  \n",
       "105   LBK nr 250 af 04/03/2024  \n",
       "\n",
       "[106 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dev set\n",
    "dev_set = pd.read_csv(\"data/dev_set.csv\").astype(str)\n",
    "dev_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize retrieval corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42593/42593 [00:00<00:00, 139300.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(rag_list):\n",
    "    # extract and preprocess text\n",
    "    corpus = [item['text'] for item in rag_list]\n",
    "    corpus = [re.sub('\\\\s{2,}', ' ', \n",
    "                     re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                           item.lower())) for item in corpus]\n",
    "\n",
    "    # remove stopwords\n",
    "    #nltk.download('punkt')\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    corpus = [' '.join(word for word in text.split() \n",
    "                      if word not in stop_words) for text in tqdm(corpus)]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = preprocess(rag_list)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse retrieval pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvor overføres indeståender til i Særlig Pensionsopsparing (SP), som det ikke har været muligt at udbetale på grund af kontohaverens forhold? \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Indeståender i Særlig Pensionsopsparing (SP), som det ikke har været muligt at udbetale på grund af kontohaverens forhold, overføres pr. 1. oktober 2010 til Arbejdsmarkedets Tillægspension til forvaltning. En kontohaver eller boet efter en afdød kontohaver, der kan dokumentere et krav på et indestående, kan i perioden fra den 1. maj 2010 til den 1. maj 2015 henvende sig til Arbejdsmarkedets Tillægspension og få udbetalt tilgodehavendet. Overførte indeståender som nævnt i stk. 1, som kontohaver eller boet efter en kontohaver ikke senest den 30. april 2015 har anmodet om at få udbetalt, tilfalder Arbejdsmarkedets Tillægspension. Beskæftigelsesministeren fastsætter regler om nedsættelse af fleksydelse på grund af pensionsopsparing. Arbejdsmarkedets Tillægspension fastsætter omkostningsprocenter og gebyrer i forbindelse med forvaltning og administration af SP.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sparse_retrieval(question, corpus_embeddings=X, corpus=rag_list, vectorizer=vectorizer, k=3, max_tokens=800):\n",
    "    \"\"\"\n",
    "    Function that takes a question and returns a list of paragraphs that are most relevant to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # preprocess and vectorize question\n",
    "    question_processed = [re.sub('\\\\s{2,}', ' ', \n",
    "                               re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                                     question.lower()))]\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('danish'))\n",
    "    question_processed = [' '.join(word for word in text.split() \n",
    "                                 if word not in stop_words) for text in question_processed]\n",
    "    \n",
    "    # embed question\n",
    "    question_vector = vectorizer.transform(question_processed)\n",
    "\n",
    "    # calculate cosine similarity\n",
    "    sparse_retrieval = corpus_embeddings.dot(question_vector.T).toarray()\n",
    "\n",
    "    # get top k paragraphs\n",
    "    top_k = np.argsort(sparse_retrieval.flatten())[-k:]\n",
    "\n",
    "    # truncate context to approximate token limit\n",
    "    context = '\\n'.join([corpus[i]['text'] for i in reversed(top_k)])\n",
    "    context_words = context.split()[:max_tokens]\n",
    "    return ' '.join(context_words)\n",
    "\n",
    "# check if it works using a random question from the dev set\n",
    "random_question = dev_set.iloc[np.random.randint(0, len(dev_set))]['question']\n",
    "print(random_question, '\\n')\n",
    "sparse_retrieval(random_question, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus matrix shape: (42593, 107999)\n",
      "question vector shape: (1, 107999)\n",
      "question vector transpose shape: (107999, 1)\n",
      "sparse retrieval shape: (42593, 1)\n"
     ]
    }
   ],
   "source": [
    "# inspect the dimensions in more detail to understand the sparse retrieval\n",
    "question_processed = [re.sub('\\\\s{2,}', ' ', \n",
    "                               re.sub('\\\\W|[0-9]|§', ' ',\n",
    "                                     random_question.lower()))]\n",
    "question_vector = vectorizer.transform(question_processed)\n",
    "\n",
    "print(f'corpus matrix shape: {X.toarray().shape}')\n",
    "print(f'question vector shape: {question_vector.toarray().shape}')\n",
    "print(f'question vector transpose shape: {question_vector.T.toarray().shape}')\n",
    "print(f'sparse retrieval shape: {X.dot(question_vector.T).toarray().shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense. The corpus matrix, X, covers 42593 paragraphs (rows) with 107999 unique terms (columns). The query vector describes just one document with the same vocabulary (i.e., 107999 unique terms), and when transposed the unique terms become the rows. The sparse retrieval is the dot product between the corpus matrix and the query vector, which results in a 42593 by 1 vector, i.e., the cosine similarity between the query and each paragraph in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean norm: 0.9996\n",
      "Std norm: 0.0200\n"
     ]
    }
   ],
   "source": [
    "# calculate L2 norms of each document vector\n",
    "norms = np.sqrt((X.toarray()**2).sum(axis=1))\n",
    "print(f\"Mean norm: {norms.mean():.4f}\")\n",
    "print(f\"Std norm: {norms.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the vectors in the corpus matrix are tf-idf normalized. Hence, the cosine similarity is equivalent to the dot product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG retriever on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pipeline and move the model to MPS\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"KennethTM/gpt-neo-1.3B-danish\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I aktieselskaber kan alle anpartshavere forlange indkaldelse til en ekstraordinær generalforsamling.\n",
      "Forslag til vedtægtsændringer skal for at kunne behandles på en ekstraordinær generalforsamling være modtaget hos bestyrelsen senest 3 uger før, generalforsamlingen skal holdes.\n",
      "Ethvert kapitalejer, der kan anmelde sin interesse ifølge §§ 5 og 6, skal anmelde sit krav senest 3 uger før afholdelse af den ekstraordinære generalforsamling, medmindre der foreligger særlige omstændigheder, herunder, men ikke begrænset til, tilfælde, hvor kapitalejeren i øvrigt er forhindret i at anmelde sit krav. \n",
      "\n",
      "\n",
      " Kan ministeren fastsætte regler om beskyttelse af natur og miljø efter lov nr. 316 af 25. maj 2004 om naturbeskyttelse og lov nr. 316 af 25. maj 2004 om beskyttelse af havmiljøet i danske farvande?Det er muligt at tilkøbe en lang række ydelser fra den offentlige myndighed, fx hjælp til sagsbehandling, borgeroverdragelse, rådgivning af myndigheder og behandling af klager.\n",
      "Vedligeholdelse af det offentlige. Med en årlig besparelse på 6.500 kr. på drift af den offentlige myndighed. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test how the model performs on random questions from the dev set\n",
    "for question in random.sample(list(dev_set['question']), 2):\n",
    "    \n",
    "    # get top k paragraphs\n",
    "    top_k = sparse_retrieval(question, X, k=3)\n",
    "    \n",
    "    # get the context\n",
    "    context = ' '.join([rag_list[i]['text'] for i in top_k])\n",
    "    \n",
    "    # query the model\n",
    "    prompt = f\"Relevante paragraffer: {context}\\n\\nSpørgsmål: {question} \\n\\nSvar: \"\n",
    "    generated_text = generator(prompt, max_new_tokens=100, pad_token_id=50256)\n",
    "    print(generated_text[0]['generated_text'][len(prompt) + 1:], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed question 1 of 106\n",
      "Processed question 2 of 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1267 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed question 3 of 106\n",
      "Processed question 4 of 106\n",
      "Processed question 5 of 106\n",
      "Processed question 6 of 106\n",
      "Processed question 7 of 106\n",
      "Processed question 8 of 106\n",
      "Processed question 9 of 106\n",
      "Processed question 10 of 106\n",
      "Processed question 11 of 106\n",
      "Processed question 12 of 106\n",
      "Processed question 13 of 106\n",
      "Processed question 14 of 106\n",
      "Processed question 15 of 106\n",
      "Processed question 16 of 106\n",
      "Processed question 17 of 106\n",
      "Processed question 18 of 106\n",
      "Processed question 19 of 106\n",
      "Processed question 20 of 106\n",
      "Processed question 21 of 106\n",
      "Processed question 22 of 106\n",
      "Processed question 23 of 106\n",
      "Processed question 24 of 106\n",
      "Processed question 25 of 106\n",
      "Processed question 26 of 106\n",
      "Processed question 27 of 106\n",
      "Processed question 28 of 106\n",
      "Processed question 29 of 106\n",
      "Processed question 30 of 106\n",
      "Processed question 31 of 106\n",
      "Processed question 32 of 106\n",
      "Processed question 33 of 106\n",
      "Processed question 34 of 106\n",
      "Processed question 35 of 106\n",
      "Processed question 36 of 106\n",
      "Processed question 37 of 106\n",
      "Processed question 38 of 106\n",
      "Processed question 39 of 106\n",
      "Processed question 40 of 106\n",
      "Processed question 41 of 106\n",
      "Processed question 42 of 106\n",
      "Processed question 43 of 106\n",
      "Processed question 44 of 106\n",
      "Processed question 45 of 106\n",
      "Processed question 46 of 106\n",
      "Processed question 47 of 106\n",
      "Processed question 48 of 106\n",
      "Processed question 49 of 106\n",
      "Processed question 50 of 106\n",
      "Processed question 51 of 106\n",
      "Processed question 52 of 106\n",
      "Processed question 53 of 106\n",
      "Processed question 54 of 106\n",
      "Processed question 55 of 106\n",
      "Processed question 56 of 106\n",
      "Processed question 57 of 106\n",
      "Processed question 58 of 106\n",
      "Processed question 59 of 106\n",
      "Processed question 60 of 106\n",
      "Processed question 61 of 106\n",
      "Processed question 62 of 106\n",
      "Processed question 63 of 106\n",
      "Processed question 64 of 106\n",
      "Processed question 65 of 106\n",
      "Processed question 66 of 106\n",
      "Processed question 67 of 106\n",
      "Processed question 68 of 106\n",
      "Processed question 69 of 106\n",
      "Processed question 70 of 106\n",
      "Processed question 71 of 106\n",
      "Processed question 72 of 106\n",
      "Processed question 73 of 106\n",
      "Processed question 74 of 106\n",
      "Processed question 75 of 106\n",
      "Processed question 76 of 106\n",
      "Processed question 77 of 106\n",
      "Processed question 78 of 106\n",
      "Processed question 79 of 106\n",
      "Processed question 80 of 106\n",
      "Processed question 81 of 106\n",
      "Processed question 82 of 106\n",
      "Processed question 83 of 106\n",
      "Processed question 84 of 106\n",
      "Processed question 85 of 106\n",
      "Processed question 86 of 106\n",
      "Processed question 87 of 106\n",
      "Processed question 88 of 106\n",
      "Processed question 89 of 106\n",
      "Processed question 90 of 106\n",
      "Processed question 91 of 106\n",
      "Processed question 92 of 106\n",
      "Processed question 93 of 106\n",
      "Processed question 94 of 106\n",
      "Processed question 95 of 106\n",
      "Processed question 96 of 106\n",
      "Processed question 97 of 106\n",
      "Processed question 98 of 106\n",
      "Processed question 99 of 106\n",
      "Processed question 100 of 106\n",
      "Processed question 101 of 106\n",
      "Processed question 102 of 106\n",
      "Processed question 103 of 106\n",
      "Processed question 104 of 106\n",
      "Processed question 105 of 106\n",
      "Processed question 106 of 106\n",
      "Mean BLEU score: 0.02\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'RougeScorer' and 'RougeScorer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# calculate mean scores\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean BLEU score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(bleu_scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean ROUGE score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrouge_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3505\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'RougeScorer' and 'RougeScorer'"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "\n",
    "# run through the questions in the dev set and calculate a bleu score and a rouge score\n",
    "for question, correct_answer in tqdm(zip(dev_set['question'], dev_set['answer']), total=len(dev_set), leave=False):\n",
    "\n",
    "    # run through RAG pipeline and generate answer\n",
    "    top_k = sparse_retrieval(question, X, k=3)\n",
    "    context = ' '.join([rag_list[i]['text'] for i in top_k])\n",
    "    prompt = f\"Relevante paragraffer: {context}\\n\\nSpørgsmål: {question} \\n\\nSvar: \"\n",
    "    generated_text = generator(prompt, max_new_tokens=100, pad_token_id=50256)\n",
    "\n",
    "    # store answer and scores\n",
    "    generated_answer = generated_text[0]['generated_text'][len(prompt) + 1:]\n",
    "    answers.append(generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 106, 106)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bleu_scores), len(rouge_scores), len(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU score: 0.02\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean BLEU score: {np.mean(bleu_scores):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_scores(answers, dev_set):\n",
    "    bleu_scores = []\n",
    "    for i in range(len(answers)):\n",
    "        reference = str(dev_set['answer'].iloc[i]).split()\n",
    "        hypothesis = answers[i].split()\n",
    "        bleu_scores.append(sentence_bleu([reference], hypothesis, smoothing_function=SmoothingFunction().method1))\n",
    "    return np.mean(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge1': 0.041666666666666664,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.041666666666666664},\n",
       " {'rouge1': 0.08333333333333334,\n",
       "  'rouge2': 0.016949152542372885,\n",
       "  'rougeL': 0.06666666666666667},\n",
       " {'rouge1': 0.288659793814433,\n",
       "  'rouge2': 0.21052631578947367,\n",
       "  'rougeL': 0.288659793814433},\n",
       " {'rouge1': 0.27722772277227725,\n",
       "  'rouge2': 0.1414141414141414,\n",
       "  'rougeL': 0.19801980198019803},\n",
       " {'rouge1': 0.17857142857142858,\n",
       "  'rouge2': 0.07272727272727272,\n",
       "  'rougeL': 0.16071428571428567},\n",
       " {'rouge1': 0.27419354838709675,\n",
       "  'rouge2': 0.13114754098360656,\n",
       "  'rougeL': 0.22580645161290322},\n",
       " {'rouge1': 0.23636363636363633,\n",
       "  'rouge2': 0.09259259259259259,\n",
       "  'rougeL': 0.1272727272727273},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.056603773584905655,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.03773584905660377},\n",
       " {'rouge1': 0.15267175572519084,\n",
       "  'rouge2': 0.015503875968992248,\n",
       "  'rougeL': 0.09160305343511449},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.27586206896551724,\n",
       "  'rouge2': 0.22807017543859648,\n",
       "  'rougeL': 0.20689655172413793},\n",
       " {'rouge1': 0.14678899082568805,\n",
       "  'rouge2': 0.056074766355140186,\n",
       "  'rougeL': 0.12844036697247704},\n",
       " {'rouge1': 0.10204081632653061, 'rouge2': 0.0, 'rougeL': 0.0816326530612245},\n",
       " {'rouge1': 0.24347826086956523,\n",
       "  'rouge2': 0.08849557522123894,\n",
       "  'rougeL': 0.1565217391304348},\n",
       " {'rouge1': 0.22972972972972974,\n",
       "  'rouge2': 0.027397260273972605,\n",
       "  'rougeL': 0.16216216216216217},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.1111111111111111,\n",
       "  'rouge2': 0.05714285714285715,\n",
       "  'rougeL': 0.1111111111111111},\n",
       " {'rouge1': 0.02061855670103093, 'rouge2': 0.0, 'rougeL': 0.02061855670103093},\n",
       " {'rouge1': 0.0970873786407767, 'rouge2': 0.0, 'rougeL': 0.0970873786407767},\n",
       " {'rouge1': 0.06741573033707865,\n",
       "  'rouge2': 0.04597701149425288,\n",
       "  'rougeL': 0.06741573033707865},\n",
       " {'rouge1': 0.06593406593406592,\n",
       "  'rouge2': 0.0449438202247191,\n",
       "  'rougeL': 0.06593406593406592},\n",
       " {'rouge1': 0.21276595744680848,\n",
       "  'rouge2': 0.1956521739130435,\n",
       "  'rougeL': 0.21276595744680848},\n",
       " {'rouge1': 0.1956521739130435,\n",
       "  'rouge2': 0.17777777777777776,\n",
       "  'rougeL': 0.1956521739130435},\n",
       " {'rouge1': 0.3132530120481928,\n",
       "  'rouge2': 0.29629629629629634,\n",
       "  'rougeL': 0.3132530120481928},\n",
       " {'rouge1': 0.14285714285714285,\n",
       "  'rouge2': 0.07317073170731707,\n",
       "  'rougeL': 0.11904761904761903},\n",
       " {'rouge1': 0.04597701149425287, 'rouge2': 0.0, 'rougeL': 0.04597701149425287},\n",
       " {'rouge1': 0.04545454545454545,\n",
       "  'rouge2': 0.023255813953488372,\n",
       "  'rougeL': 0.04545454545454545},\n",
       " {'rouge1': 0.22916666666666669,\n",
       "  'rouge2': 0.19148936170212766,\n",
       "  'rougeL': 0.14583333333333331},\n",
       " {'rouge1': 0.20560747663551404,\n",
       "  'rouge2': 0.13333333333333333,\n",
       "  'rougeL': 0.18691588785046728},\n",
       " {'rouge1': 0.11320754716981131,\n",
       "  'rouge2': 0.038461538461538464,\n",
       "  'rougeL': 0.11320754716981131},\n",
       " {'rouge1': 0.10309278350515462,\n",
       "  'rouge2': 0.042105263157894736,\n",
       "  'rougeL': 0.06185567010309278},\n",
       " {'rouge1': 0.02666666666666666, 'rouge2': 0.0, 'rougeL': 0.02666666666666666},\n",
       " {'rouge1': 0.022727272727272728,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.022727272727272728},\n",
       " {'rouge1': 0.08247422680412371,\n",
       "  'rouge2': 0.042105263157894736,\n",
       "  'rougeL': 0.08247422680412371},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.1346153846153846, 'rouge2': 0.0, 'rougeL': 0.09615384615384615},\n",
       " {'rouge1': 0.023529411764705882,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.023529411764705882},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.16091954022988506,\n",
       "  'rouge2': 0.1411764705882353,\n",
       "  'rougeL': 0.16091954022988506},\n",
       " {'rouge1': 0.3791469194312796,\n",
       "  'rouge2': 0.1818181818181818,\n",
       "  'rougeL': 0.23696682464454974},\n",
       " {'rouge1': 0.2446043165467626,\n",
       "  'rouge2': 0.029197080291970802,\n",
       "  'rougeL': 0.11510791366906475},\n",
       " {'rouge1': 0.22068965517241382,\n",
       "  'rouge2': 0.013986013986013986,\n",
       "  'rougeL': 0.12413793103448273},\n",
       " {'rouge1': 0.3493975903614458,\n",
       "  'rouge2': 0.13414634146341461,\n",
       "  'rougeL': 0.21686746987951808},\n",
       " {'rouge1': 0.26666666666666666, 'rouge2': 0.0, 'rougeL': 0.11851851851851852},\n",
       " {'rouge1': 0.31999999999999995,\n",
       "  'rouge2': 0.1081081081081081,\n",
       "  'rougeL': 0.19999999999999998},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.02380952380952381, 'rouge2': 0.0, 'rougeL': 0.02380952380952381},\n",
       " {'rouge1': 0.10526315789473684,\n",
       "  'rouge2': 0.08108108108108109,\n",
       "  'rougeL': 0.10526315789473684},\n",
       " {'rouge1': 0.0689655172413793,\n",
       "  'rouge2': 0.047058823529411764,\n",
       "  'rougeL': 0.0689655172413793},\n",
       " {'rouge1': 0.1346153846153846, 'rouge2': 0.0, 'rougeL': 0.07692307692307693},\n",
       " {'rouge1': 0.09756097560975609,\n",
       "  'rouge2': 0.025,\n",
       "  'rougeL': 0.07317073170731707},\n",
       " {'rouge1': 0.5714285714285714,\n",
       "  'rouge2': 0.5496183206106869,\n",
       "  'rougeL': 0.5714285714285714},\n",
       " {'rouge1': 0.06521739130434784,\n",
       "  'rouge2': 0.02222222222222222,\n",
       "  'rougeL': 0.06521739130434784},\n",
       " {'rouge1': 0.14473684210526314,\n",
       "  'rouge2': 0.013333333333333332,\n",
       "  'rougeL': 0.07894736842105263},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.10619469026548672, 'rouge2': 0.0, 'rougeL': 0.07079646017699115},\n",
       " {'rouge1': 0.0392156862745098, 'rouge2': 0.0, 'rougeL': 0.0392156862745098},\n",
       " {'rouge1': 0.14457831325301204,\n",
       "  'rouge2': 0.07407407407407407,\n",
       "  'rougeL': 0.14457831325301204},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.19047619047619047,\n",
       "  'rouge2': 0.013793103448275862,\n",
       "  'rougeL': 0.1496598639455782},\n",
       " {'rouge1': 0.19607843137254902,\n",
       "  'rouge2': 0.013245033112582781,\n",
       "  'rougeL': 0.1045751633986928},\n",
       " {'rouge1': 0.18487394957983191,\n",
       "  'rouge2': 0.05128205128205127,\n",
       "  'rougeL': 0.13445378151260504},\n",
       " {'rouge1': 0.12612612612612611, 'rouge2': 0.0, 'rougeL': 0.0900900900900901},\n",
       " {'rouge1': 0.07766990291262135,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.058252427184466014},\n",
       " {'rouge1': 0.16666666666666666,\n",
       "  'rouge2': 0.12195121951219512,\n",
       "  'rougeL': 0.16666666666666666},\n",
       " {'rouge1': 0.12244897959183673,\n",
       "  'rouge2': 0.08333333333333334,\n",
       "  'rougeL': 0.12244897959183673},\n",
       " {'rouge1': 0.21052631578947364,\n",
       "  'rouge2': 0.035398230088495575,\n",
       "  'rougeL': 0.09649122807017543},\n",
       " {'rouge1': 0.08771929824561403,\n",
       "  'rouge2': 0.017857142857142856,\n",
       "  'rougeL': 0.05263157894736841},\n",
       " {'rouge1': 0.13513513513513511,\n",
       "  'rouge2': 0.1111111111111111,\n",
       "  'rougeL': 0.13513513513513511},\n",
       " {'rouge1': 0.16666666666666666,\n",
       "  'rouge2': 0.07317073170731707,\n",
       "  'rougeL': 0.11904761904761905},\n",
       " {'rouge1': 0.203125,\n",
       "  'rouge2': 0.015873015873015872,\n",
       "  'rougeL': 0.15624999999999997},\n",
       " {'rouge1': 0.01869158878504673, 'rouge2': 0.0, 'rougeL': 0.01869158878504673},\n",
       " {'rouge1': 0.01941747572815534, 'rouge2': 0.0, 'rougeL': 0.01941747572815534},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.04545454545454545, 'rouge2': 0.0, 'rougeL': 0.04545454545454545},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.09756097560975609,\n",
       "  'rouge2': 0.07500000000000001,\n",
       "  'rougeL': 0.09756097560975609},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.02222222222222222, 'rouge2': 0.0, 'rougeL': 0.02222222222222222},\n",
       " {'rouge1': 0.02197802197802198, 'rouge2': 0.0, 'rougeL': 0.02197802197802198},\n",
       " {'rouge1': 0.06896551724137931,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.045977011494252866},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.022988505747126436,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.022988505747126436},\n",
       " {'rouge1': 0.04395604395604396, 'rouge2': 0.0, 'rougeL': 0.02197802197802198},\n",
       " {'rouge1': 0.08791208791208792,\n",
       "  'rouge2': 0.06741573033707865,\n",
       "  'rougeL': 0.08791208791208792},\n",
       " {'rouge1': 0.06382978723404255,\n",
       "  'rouge2': 0.04347826086956522,\n",
       "  'rougeL': 0.06382978723404255},\n",
       " {'rouge1': 0.023809523809523808,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.023809523809523808},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.06593406593406594, 'rouge2': 0.0, 'rougeL': 0.06593406593406594},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.02173913043478261, 'rouge2': 0.0, 'rougeL': 0.02173913043478261},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.022988505747126436,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.022988505747126436},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " {'rouge1': 0.025974025974025972,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.025974025974025972},\n",
       " {'rouge1': 0.022988505747126436,\n",
       "  'rouge2': 0.0,\n",
       "  'rougeL': 0.022988505747126436},\n",
       " {'rouge1': 0.04081632653061225, 'rouge2': 0.0, 'rougeL': 0.04081632653061225},\n",
       " {'rouge1': 0.09523809523809523,\n",
       "  'rouge2': 0.07317073170731708,\n",
       "  'rougeL': 0.09523809523809523},\n",
       " {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_rouge_scores(answers, dev_set):\n",
    "    # Calculate actual ROUGE scores from the scorer objects\n",
    "    rouge_scores_dict = []\n",
    "    for i in range(len(answers)):\n",
    "        # Skip any entries where answers are not strings\n",
    "        if isinstance(dev_set['answer'].iloc[i], float) or isinstance(answers[i], float):\n",
    "            continue\n",
    "            \n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "        scores = scorer.score(target=dev_set['answer'].iloc[i], prediction=answers[i])\n",
    "        rouge_scores_dict.append({\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure\n",
    "        })\n",
    "\n",
    "    # get the average of the different rouge scores\n",
    "    rouge1_scores = [score['rouge1'] for score in rouge_scores_dict]\n",
    "    rouge2_scores = [score['rouge2'] for score in rouge_scores_dict]\n",
    "    rougeL_scores = [score['rougeL'] for score in rouge_scores_dict]\n",
    "    \n",
    "    return rouge1_scores, rouge2_scores, rougeL_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROUGE1 score: 0.10\n",
      "Mean ROUGE2 score: 0.04\n",
      "Mean ROUGE-L score: 0.08\n"
     ]
    }
   ],
   "source": [
    "# get the average of the different rouge scores\n",
    "rouge1_scores = [score['rouge1'] for score in rouge_scores_dict]\n",
    "rouge2_scores = [score['rouge2'] for score in rouge_scores_dict]\n",
    "rougeL_scores = [score['rougeL'] for score in rouge_scores_dict]\n",
    "print(f'Mean ROUGE1 score: {np.mean(rouge1_scores):.2f}')\n",
    "print(f'Mean ROUGE2 score: {np.mean(rouge2_scores):.2f}')\n",
    "print(f'Mean ROUGE-L score: {np.mean(rougeL_scores):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fraction.__new__() got an unexpected keyword argument '_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     reference \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      5\u001b[0m     hypothesis \u001b[38;5;241m=\u001b[39m question\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m----> 6\u001b[0m     bleu_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbleu_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean BLEU score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(bleu_list)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/translate/bleu_score.py:107\u001b[0m, in \u001b[0;36msentence_bleu\u001b[0;34m(references, hypothesis, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_bleu\u001b[39m(\n\u001b[1;32m     21\u001b[0m     references,\n\u001b[1;32m     22\u001b[0m     hypothesis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     auto_reweigh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m ):\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Calculate BLEU score (Bilingual Evaluation Understudy) from\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    :rtype: float / list(float)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorpus_bleu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_reweigh\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/translate/bleu_score.py:210\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m references, hypothesis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(list_of_references, hypotheses):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# For each order of ngram, calculate the numerator and\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# denominator for the corpus-level modified precision.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_weight_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 210\u001b[0m         p_i \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m         p_numerators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mnumerator\n\u001b[1;32m    212\u001b[0m         p_denominators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mdenominator\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/translate/bleu_score.py:368\u001b[0m, in \u001b[0;36mmodified_precision\u001b[0;34m(references, hypothesis, n)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# Usually this happens when the ngram order is > len(reference).\u001b[39;00m\n\u001b[1;32m    366\u001b[0m denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28msum\u001b[39m(counts\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenominator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fraction.__new__() got an unexpected keyword argument '_normalize'"
     ]
    }
   ],
   "source": [
    "# let's just calculate the bleu score btw questions and answers to get it to work'\n",
    "bleu_list = []\n",
    "for question, answer in zip(dev_set['question'], dev_set['answer']):\n",
    "    reference = answer.split()\n",
    "    hypothesis = question.split()\n",
    "    bleu_list.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis))\n",
    "\n",
    "print(f'Mean BLEU score: {np.mean(bleu_list):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
