{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# For modules loading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, T5ForConditionalGeneration\n",
    "import torch\n",
    "from itertools import cycle\n",
    "import os \n",
    "from transformers import pipeline\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# For BM25\n",
    "import src.bm25_IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set = pd.read_csv('devset/dev_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained T5 model and tokenizer\n",
    "model_name = \"strombergnlp/dant5-large\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevante paragraffer: I denne lov forstås ved: Ejerlejlighed: En lejlighed samt andre særskilt afgrænsede husrum, der er registreret som en ejerlejlighed efter reglerne i denne lov eller tidligere gældende lovgivning. Ejerlejlighed: En lejlighed samt andre særskilt afgrænsede husrum, der er registreret som en ejerlejlighed efter reglerne i denne lov eller tidligere gældende lovgivning. Ejerforening: Et obligatorisk fællesskab bestående af samtlige ejere af ejerlejligheder på en ejendom. Ejendom: En bestemt fast ejendom i tinglysningslovens forstand. Bygning: En fast konstruktion med vægge og tag beregnet til beboelse, erhverv eller opbevaring. Opdeling: Oprettelse af ejerlejligheder i bygninger på en ejendom. Videreopdeling: Oprettelse af flere ejerlejligheder i en eksisterende ejerlejlighed. Ændring: Anden ændring af en ejerlejligheds registrering end en videreopdeling.\n",
      "Hver ejerlejlighed anses som en selvstændig fast ejendom. Hver ejerlejlighed er identificeret ved eget identifikationsnummer.\n",
      "Ejeren af en ejerlejlighed har sammen med andre ejere af lejligheder ejendomsret til grunden, fælles bestanddele og tilbehør m.v. efter et fordelingstal, der fastsættes som en brøkdel. Til lejligheden hører i samme forhold rettigheder og forpligtelser for ejeren som medlem af ejerforeningen. De i stk. 1 og 2 omhandlede rettigheder og forpligtelser kan ikke adskilles fra ejendomsretten til lejligheden.\n",
      "Spørgsmål: Hvad har ejeren af en ejerlejlighed, sammen med andre ejere af lejligheder, ejendomsret til?\n",
      "Indsæt svar her baseret på de relevante paragraffer: \n",
      "\n",
      ": Ejerlejlighed: En ejerlejlighed, der er registreret som en ejerlejlighed efter reglerne i denne lov eller tidligere gældende lovgivning. Ejerlejlighed: En ejerlejlighed, der er registreret som en ejerlejlighed efter reglerne i denne lov eller tidligere gældende lovgivning. Ejerlejlighed: En ejerlejlighed, der er registreret som en ejerlejlighed efter reglerne i denne lov eller tidligere gældende lovgivning. Ejerlejlighed: En ejerlejlighed. Ejerlejlighed: En ejerlejlighed.\n"
     ]
    }
   ],
   "source": [
    "#for question, documents in tqdm(zip(dev_set['question, str'], dev_set['tf_idf_k1']), desc='Answering questions'):\n",
    "\n",
    "# Example question and context\n",
    "question = dev_set.loc[0, 'question']\n",
    "documents = sparse_retrieval(question, tf_idf_corpus, k=3)\n",
    "\n",
    "# Format the input for T5\n",
    "input_text = f\"Relevante paragraffer: {documents}\\nSpørgsmål: {question}\\nIndsæt svar her baseret på de relevante paragraffer:\"\n",
    "\n",
    "# Tokenize the input and generate an answer\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "\n",
    "max_length = len(input_ids[0]) + 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        # generation set to stop at ' Spørgsmål' as it otherwise just repeats itself (think it's because we don't sample)\n",
    "        eos_token_id=tokenizer.encode(' Spørgsmål')[0]\n",
    "    )\n",
    "\n",
    "# Decode and print the generated answer\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip(' Spørgsmål')\n",
    "print(input_text, '\\n')\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
