{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CLS embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating embedding corpus with the 'vesteinn/DanskBERT'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "model = AutoModel.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "cls_embeddings = []\n",
    "idx = 0\n",
    "for item in tqdm(rag_list):\n",
    "    # doing a try and except as some paragraphs may exceed the context window of the BERT (I believe)\n",
    "    try:\n",
    "        # tokenize texts\n",
    "        input_ids = tokenizer.encode(item['text'], return_tensors='pt')\n",
    "        # run through BERT\n",
    "        with torch.no_grad():  # disable gradient computation for inference\n",
    "            outputs = model(input_ids)\n",
    "        # extract cls-token\n",
    "        cls_vector = outputs.last_hidden_state[:, 0, :]\n",
    "        # add cls-vector to list of embeddings\n",
    "        cls_embeddings.append(cls_vector)\n",
    "    except:\n",
    "        # if error then count errors with this\n",
    "        cls_embeddings.append(torch.zeros(768))\n",
    "        idx += 1\n",
    "print(f'{idx} no. of errors')\n",
    "# concatenate list into torch tensor\n",
    "cls_embeddings_tensor_DanskBERT = torch.cat(cls_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the CLS-tensor\n",
    "torch.save(cls_embeddings_tensor_DanskBERT, '/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_DanskBERT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the CLS-tensor\n",
    "cls_embeddings_tensor_DanskBERT = torch.load('/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/cls_embeddings_DanskBERT.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create max-pool embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating max-pooled embedding corpus with the 'vesteinn/DanskBERT'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "model = AutoModel.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "max_embeddings = []\n",
    "idx = 0\n",
    "for item in tqdm(rag_list):\n",
    "    # doing a try and except as some paragraphs may exceed the context window of the BERT (I believe)\n",
    "    try:\n",
    "        # tokenize texts\n",
    "        input_ids = tokenizer.encode(item['text'], return_tensors='pt')\n",
    "        # run through BERT\n",
    "        with torch.no_grad():  # disable gradient computation for inference\n",
    "            outputs = model(input_ids)\n",
    "        # extract cls-token\n",
    "        max_pooled_embedding = torch.max(outputs.last_hidden_state, dim=1)[0]\n",
    "        # add cls-vector to list of embeddings\n",
    "        max_embeddings.append(max_pooled_embedding)\n",
    "    except:\n",
    "        # if error then count errors with this\n",
    "        idx += 1\n",
    "print(f'{idx} no. of errors')\n",
    "# concatenate list into torch tensor\n",
    "max_embeddings_tensor_DanskBERT = torch.cat(max_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the max pool tensor\n",
    "#torch.save(max_embeddings_tensor_DanskBERT, '/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/max_embeddings_DanskBERT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the max pool tensor\n",
    "#cls_embeddings_tensor_DanskBERT = torch.load('/Users/adamwagnerhoegh/Documents/SODAS/sem3/nlp_itu/max_embeddings_DanskBERT.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mean-pooled embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating mean-pooled embedding corpus with the 'vesteinn/DanskBERT'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "# model = AutoModel.from_pretrained(\"vesteinn/DanskBERT\")\n",
    "\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# cls_embeddings = []\n",
    "\n",
    "# idx = 0\n",
    "\n",
    "# for item in tqdm(rag_list):\n",
    "#     # doing a try and except as some paragraphs may exceed the context window of the BERT (I believe)\n",
    "#     try:\n",
    "#         # tokenize texts\n",
    "#         input_ids = tokenizer.encode(item['text'], return_tensors='pt')\n",
    "#         # run through BERT\n",
    "#         with torch.no_grad():  # disable gradient computation for inference\n",
    "#             outputs = model(input_ids)\n",
    "#         # extract cls-token\n",
    "#         mean_pooled_embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "#         # add cls-vector to list of embeddings\n",
    "#         cls_embeddings.append(max_pooled_embedding)\n",
    "#     except:\n",
    "#         # if error then count errors with this\n",
    "#         idx += 1\n",
    "\n",
    "# print(f'{idx} no. of errors')\n",
    "\n",
    "# # concatenate list into torch tensor\n",
    "# cls_embeddings_tensor_DanskBERT = torch.cat(cls_embeddings, dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
