{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamwagnerhoegh/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# For modules loading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, T5ForConditionalGeneration\n",
    "import torch\n",
    "from itertools import cycle\n",
    "import os \n",
    "from transformers import pipeline\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# For BM25\n",
    "import src.bm25_IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained T5 model and tokenizer\n",
    "model_name = \"strombergnlp/dant5-large\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for question, documents in tqdm(zip(dev_set['question, str'], dev_set['tf_idf_k1']), desc='Answering questions'):\n",
    "\n",
    "# Example question and context\n",
    "question = dev_set.loc[0, 'question, str']\n",
    "documents = sparse_retrieval(question, X, k=3)\n",
    "\n",
    "# Format the input for T5\n",
    "input_text = f\"Relevante paragraffer: {documents}\\nSpørgsmål: {question}\\nIndsæt svar her baseret på de relevante paragraffer:\"\n",
    "\n",
    "# Tokenize the input and generate an answer\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "\n",
    "max_length = len(input_ids[0]) + 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        # generation set to stop at '.' as it otherwise just repeats itself (think it's because we don't sample)\n",
    "        eos_token_id=tokenizer.encode(' Spørgsmål')[0]\n",
    "    )\n",
    "\n",
    "# Decode and print the generated answer\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip(' Spørgsmål')\n",
    "print(input_text, '\\n')\n",
    "print(\"Svar:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
